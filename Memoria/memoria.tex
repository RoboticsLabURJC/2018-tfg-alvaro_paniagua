%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Plantilla de memoria en LaTeX para la ETSIT - Universidad Rey Juan Carlos
%%
%% Por Gregorio Robles <grex arroba gsyc.urjc.es>
%%     Grupo de Sistemas y Comunicaciones
%%     Escuela Técnica Superior de Ingenieros de Telecomunicación
%%     Universidad Rey Juan Carlos
%% (muchas ideas tomadas de Internet, colegas del GSyC, antiguos alumnos...
%%  etc. Muchas gracias a todos)
%%
%% La última versión de esta plantilla está siempre disponible en:
%%     https://github.com/gregoriorobles/plantilla-memoria
%%
%% Para obtener PDF, ejecuta en la shell:
%%   make
%% (las imágenes deben ir en PNG o JPG)

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[a4paper, 12pt]{book}
%\usepackage[T1]{fontenc}

\usepackage[a4paper, left=2.5cm, right=2.5cm, top=3cm, bottom=3cm]{geometry}
\usepackage{times}
\usepackage[table,xcdraw]{xcolor}
\usepackage{hyperref} 
\usepackage[latin1]{inputenc}
\usepackage[spanish]{babel} % Comenta esta línea si tu memoria es en inglés
\usepackage{url}
%\usepackage[dvipdfm]{graphicx}
\usepackage{graphicx}
\usepackage{tabularx}
\usepackage{longtable}
\usepackage{listings}
\usepackage{float}  %% H para posicionar figuras
\usepackage[nottoc, notlot, notlof, notindex]{tocbibind} %% Opciones de índice
\usepackage{latexsym}  %% Logo LaTeX


\title{Memoria del Proyecto}
\author{Álvaro Paniagua Tena}

\renewcommand{\baselinestretch}{1.5}  %% Interlineado

\begin{document}

\renewcommand{\refname}{Bibliografía}  %% Renombrando
\renewcommand{\appendixname}{Apéndice}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% PORTADA

\begin{titlepage}
\begin{center}
\begin{tabular}[c]{c c}
%\includegraphics[bb=0 0 194 352, scale=0.25]{logo} 
\includegraphics[scale=0.25]{img/logo_vect.PNG} &
\begin{tabular}[b]{l}
\Huge
\textsf{UNIVERSIDAD} \\
\Huge
\textsf{REY JUAN CARLOS} \\
\end{tabular}
\\
\end{tabular}

\vspace{3cm}

\Large
INGENIERÍA EN SISTEMAS AUDIOVISUALES Y MULTIMEDIA

\vspace{0.4cm}

\large
Curso Académico 2018/2019

\vspace{0.8cm}

Trabajo Fin de Grado

\vspace{2.5cm}

\Large
WEBSIM \\
SIMULADOR DE ROBOTS CON TECNOLOGÍAS WEB VR

\vspace{4cm}

\large
Autor : Álvaro Paniagua Tena \\
Tutor : Dr. Jose María Cañas Plaza\\
Co-tutor: Dr. Jesús González Barahona
\end{center}
\end{titlepage}

\newpage
\mbox{}
\thispagestyle{empty} % para que no se numere esta pagina


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%% Para firmar
\clearpage
\pagenumbering{gobble}
\chapter*{}

\vspace{-4cm}
\begin{center}
\LARGE
\emph{Trabajo Fin de Grado}

\vspace{1cm}
\large
Simulador de Robots con Tecnologías Web VR

\vspace{1cm}
\large
\emph{Autor :} Álvaro Paniagua Tena \\
\emph{Tutor :} Dr. Jose María Cañas Plaza \\
\emph{Co-tutor :} Dr. Jesús González Barahona

\end{center}

\vspace{1cm}
La defensa del presente Proyecto Fin de Carrera se realizó el día \qquad$\;\,$ de \qquad\qquad\qquad\qquad \newline de 2019, siendo calificada por el siguiente tribunal:


\vspace{0.5cm}
\emph{Presidente:}

\vspace{1.2cm}
\emph{Secretario:}

\vspace{1.2cm}
\emph{Vocal:}


\vspace{1.2cm}
y habiendo obtenido la siguiente calificación:

\vspace{1cm}
\emph{Calificación:}


\vspace{1cm}
\begin{flushright}
Fuenlabrada, a \qquad$\;\,$ de \qquad\qquad\qquad\qquad de 2019
\end{flushright}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%% Dedicatoria

\chapter*{}
\pagenumbering{Roman} % para comenzar la numeracion de paginas en numeros romanos
\begin{flushright}
\textit{Dedicado a \\
mis padres, familia, y a mi pareja Cristina}
\end{flushright}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%% Agradecimientos

\chapter*{Agradecimientos}
%\addcontentsline{toc}{chapter}{Agradecimientos} % si queremos que aparezca en el índice
\markboth{AGRADECIMIENTOS}{AGRADECIMIENTOS} % encabezado

En primer lugar dar las gracias a mis padres por apoyarme y animarme 
desde el primer momento. También agradecer a mis compañeros Roberto, 
Ángel y Ahmed por tantas horas de ayuda y clases particulares para que 
entendiese todo bien.\\

Gracias también a Jose María Cañas por darme la oportunidad de 
colaborar en el proyecto y a Jesús González Barahona por su ayuda para el desarrollo y mejora del proyecto.\\

Por último dar las gracias a mi pareja Cristina, has sido un gran apoyo 
en estos últimos años y sin duda me has motivado a hacer mejor las 
cosas y superarme.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%% Resumen

\chapter*{Resumen}
%\addcontentsline{toc}{chapter}{Resumen} % si queremos que aparezca en el índice
\markboth{RESUMEN}{RESUMEN} % encabezado

Este proyecto se encuadra dentro de las tecnologías web y la robótica, tiene la intención de desarrollar 
un simulador robótico con peso computacional únicamente en el cliente (\emph{WebSim}), es decir, el navegador. 
El simulador aprovecha el crecimiento de las tecnologías web y más específicamente el crecimiento de 
\emph{A-FRAME} para realidad virtual 3D.\\

Además del simulador se han desarrollado dos páginas web que permiten editar programas robóticos
en el propio navegador en lenguaje JavaScript y lenguaje visual de bloques con Blockly y ejecutarlos en el
propio simulador. Estas aplicaciones web son usadas por la plataforma \emph{Kibotics} para enseñar
a estudiantes a resolver ejercicios de visión artificial y de programación robótica.\\


Para el desarrollo del simulador \emph{WebSim} se han utilizado diversas herramientas como 
\emph{A-FRAME, A-FRAME Physics (sistema de físicas de A-FRAME), HTML5, JavaScript, jQuery, CSS3, Blockly y ACE
Editor}. Por último para la gestión de dependencias se utiliza la tecnología NPM y para el empaquetado
de la aplicación la herramienta \emph{WebPack}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%% Resumen en inglés

\chapter*{Summary}
%\addcontentsline{toc}{chapter}{Summary} % si queremos que aparezca en el índice
\markboth{SUMMARY}{SUMMARY} % encabezado



This project fits on web technologies and robotics, it intends to develop
a simulator with the computational weight in the client in the browser.
The simulator takes advantage of the growth of web technologies and more specifically the growth of
A-FRAME for virtual reality.\\

To use the simulator two web pages were developed that allow the user to write robotic programs directly on
the browser using JavaScript language or Blockly as visual block language and execute them on the
embedded simulator. These web aplications are used by Kibotics platform to teach students the basics to
solve artificial vision and robotic programming exercises.\\


Different tools have been used to develop the \emph{WebSim} simulator as \emph{A-FRAME, A-FRAME Physics system
, HTML5, JavaScript, jQuery, CSS3, Blockly and ACE Editor}. Finally, NPM technology is used to manage
dependencies and Webpack tool for the application's packaging.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% ÍNDICES %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Las buenas noticias es que los índices se generan automáticamente.
% Lo único que tienes que hacer es elegir cuáles quieren que se generen,
% y comentar/descomentar esa instrucción de LaTeX.

%%%% Índice de contenidos
\tableofcontents
%%%% Índice de figuras
\cleardoublepage
%\addcontentsline{toc}{chapter}{Lista de figuras} % para que aparezca en el indice de contenidos
\listoffigures % indice de figuras
%%%% Índice de tablas
%\cleardoublepage
%\addcontentsline{toc}{chapter}{Lista de tablas} % para que aparezca en el indice de contenidos
%\listoftables % indice de tablas


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% INTRODUCCIÓN %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\cleardoublepage
\chapter{Introducción}
\label{chap:intro} % etiqueta para poder referenciar luego en el texto con ~\ref{sec:intro}
\pagenumbering{arabic} % para empezar la numeración de página con números

La motivación de este proyecto es sustituir el simulador \emph{Gazebo} utilizado
actualmente por
la plataforma educativa \emph{JdeRobot-Kids}, ya que tiene un gran peso computacional en el servidor. Se 
busca la creación de un simulador con peso únicamente en el lado cliente, lo que permitirá escalar a un mayor 
número de usuarios simultáneos.

\section{Robótica}
\label{sec:robotica}

La robótica es la rama tecnológica involucrada en el diseño, fabricación y la utilización de 
robots. Un robot es una máquina que puede programarse para que interactúe con otros objetos y realice
de manera autónoma tareas costosas para las personas. La robótica combina, entre otras, la informática, 
electrónica, ingeniería y en los últimos años la visión artificial. Esta última es muy importante ya 
que la imagen representa un sensor muy potente (contiene mucha información) para el robot.\\


El desarrollo tecnológico ha generado cambios a nivel social facilitando a las personas
la mayoría de las tareas que llevamos a cabo a lo largo del día tanto dentro del trabajo,
como pueden ser largas cadenas automáticas de fabricación de productos o en el campo
de la medicina con la cirugía como en el ámbito del hogar como es el caso del aspirador Roomba. Esto 
ocurre gracias al avance de la rama de la robótica. Este campo tiene como objetivos el simplificar las 
tareas de las personas tanto en la vida cotidiana como en trabajos de alto riesgo mediante el uso de 
autómatas cada vez más desarrollados que sean capaces de realizar tareas más complicadas incluso de 
manera más eficaz que como la haría un humano.\\

La robótica está presente cada vez en más campos de desarrollo, hay aplicaciones que hacen uso de 
la robótica en:

\begin{itemize}
	\item Automoción, el campo de la automoción está experimentando un fuerte crecimiento en el uso
	de la robótica para el desarrollo de coches autónomos, es decir, coches autotripulados 
	(no necesitan un conductor). Una de las empresas más importantes en este sector es \emph{Waymo}, 
	es una de las empresas de Alphabet (Google) y ya está probando con coches autónomos de 
	nivel 4 que no necesitan un conductor de seguridad. El siguiente enlace muestra un vídeo de cómo
	funcionan estos coches autónomos.
	\footnote{\url{https://www.youtube.com/watch?v=aaOB-ErYq6Y}}
	
	\item Medicina, cabe destacar la existencia del robot Da Vinci (figura 
	~\ref{fig:davinci}) que se ha convertido en uno de los referentes de la cirugía. Se trata de un 
	dispositivo a través del cual se han llevado a cabo con éxito operaciones tan importantes 
	como las de cirugía transoral, esta cirugía se basa en el uso de un brazo robótico que puede 
	manejar el cirujano para la extracción de cáncer en partes de la boca y garganta de acceso difícil.

	\begin{figure}[h]
		\centering
		\includegraphics[width=9cm, height=7cm]{img/davinci.jpg}
		\caption{Imagen del robot \emph{Da Vinci} usado en operaciones quirúrgicas.}
		\label{fig:davinci}
	\end{figure}	
	
	
	\item Centros logísticos, en este campo una de las empresas más interesantes es Amazon con el uso de \emph{Drones} para la entrega de 
	artículos y sus robots \emph{Drives} que usan de manera interna para acelerar las entregas en sus 
	centros logísticos. Estos robots lo que hacen es deslizarse debajo de grandes estanterías donde se 
	encuentran los artículos y las llevan hacia los operarios o el destino indicado al robot. España es 
	actualmente	el tercer país en adoptar esta tecnología de la empresa Amazon en los centros 
	logísticos. En la siguiente Figura ~\ref{fig:amazon} se muestra los robots utilizados por la empresa Amazon.
	
	\begin{figure}[h]
		\centering
		\includegraphics[width=12cm, height=6cm]{img/amazonrobot.jpg}
		\caption{Robot utilizado en los centros logísticos de Amazon para acelerar el 
		proceso de entrega de productos.}
		\label{fig:amazon}
	\end{figure}
	
\end{itemize}


Actualmente la industria de la robótica está experimentando un gran crecimiento entrando en un buen
número de campos tecnológicos. Se prevé un aumento de la demanda de profesionales en el campo de la
robótica debido a esta diversidad y a la mayor sofisticación de los robots. Un ejemplo del grado de
avance que tienen actualmente los robots se puede ver en el vídeo \footnote{\url{https://www.youtube.com/watch?v=KEMt58ePNDs}} que presenta los
prototipos de la empresa \emph{Boston Dynamics} en el que se puede apreciar que cada vez los robots
tienen funcionalidades más complejas (minuto 9 en el vídeo).\\





\section{Robótica educativa}
\label{sec:robotica-educativa}

La robótica educativa ofrece entornos de aprendizaje basados en la actividad de los estudiantes, es 
decir, aprender el pensamiento lógico que va más allá de la programación o el diseño de robots mediante 
el uso de entornos 'simplificados' para el alumno. Además fomenta la resolución de problemas y el 
trabajo en equipo a través de recursos tecnológicos. La robótica es un campo multidisciplinar 
que conjunta el conocimiento matemático, físico y tecnológico. Por ello es un campo que conjuga
muy bien con las actuales materias educativas a la vez de ser una materia de aplicación, es decir,
los alumnos pueden ver el resultado de la aplicación de estos conocimientos en el robot.\\

Muchos proyectos han demostrado que el uso de kits de robótica para el aprendizaje de los alumnos
aumenta la capacidad de reflexión de estos, Jhon Siraj-Blatchford, profesor de la universidad de
Cambridge, en el libro \emph{Nuevas tecnologías para la educación infantil y primaria} argumenta este
tema. Además se ofrece un entorno educativo distinto al tradicional, más adaptado al mundo actual 
donde la tecnología crece a gran velocidad y donde los alumnos pueden disfrutar de un
entorno al que no tendrían acceso hasta unos estudios superiores o especializados. La figura 
~\ref{fig:lego} muestra un kit de robótica educativa en el que se ofrecen piezas para
la construcción de una serie robots simples y la herramienta para programar su funcionalidad.\\

\begin{figure}[h]
	\centering
	\includegraphics[width=10cm, height=6cm]{img/lego-wedo-2.jpg}
	\caption{Kit \emph{Lego WeDo 2} para robótica educativa.}
	\label{fig:lego}
\end{figure}

Dentro del marco de la robótica educativa cabe hablar de la plataforma \emph{Kibotics} \footnote{\url{kibotics.org}} en la cual
se enmarca el presente proyecto. Esta plataforma ofrece las herramientas necesarias para la 
enseñanza de robótica en alumnos de secundaria. La plataforma tiene intención de potenciar los 
conocimiento de robótica previos a los estudios universitarios, para ello provee de una 
infraestructura software, hardware y una colección de ejercicios que permiten el desarrollo completo 
del curso. Tiene soporte para robots tanto reales como simulados, el lenguaje de programación usado es 
Python y los robots reales utilizan el hardware de \emph{Raspberry PI}. La propuesta educativa llevada acabo 
por la plataforma se ha puesto en práctica con éxito en la Fundación Franciscanas de Montpellier. 
Actualmente la plataforma tiene como infraestructura hardware con el robot \emph{PiBot} el cual cuenta
con los siguientes componentes, cámara, sensores de ultrasonido, sensores infrarrojos y dos 
servomotores todo ello conectado a la \emph{Raspberry PI}, en la Figura 
~\ref{fig:pibot-real} se muestra una imagen de la infraestructura
hardware (PiBot). Además cuentan con el robot simulado en 
el simulador Gazebo. Lo que dota al estudiante de flexibilidad a la hora de realizar los ejercicios
fuera del horario lectivo.\\

\begin{figure}[h]
	\centering
	\includegraphics[width=9cm, height=6cm]{img/pibot.jpg}
	\caption{Robot PiBot utilizado por la plataforma JdeRobot-Kids.}
	\label{fig:pibot-real}
\end{figure}

Como se ha mencionado anteriormente, cada vez el desarrollo y sofisticación de los robots
es mayor debido a que se intenta que tengan una funcionalidad similar a la de un humano dotándolos 
incluso de \emph{inteligencia artificial}. Por tanto es cada vez más necesario profesionales mejor formados
lo que hace conveniente la formación en esta rama desde edades más tempranas.\\

En el enlace \footnote{\url{https://www.youtube.com/watch?v=FDnhMTxhddM}} se muestra un vídeo en el cual se hablan de las posibles aplicaciones de la 
robótica en la educación, en la charla se expone cómo aprenden los niños con la robótica los problemas
de la contaminación lumínica para los animales.\\





\section{Tecnologías Web}
\label{sec:tec-web}



Una de las ventajas principales que ofrecen las
tecnologías web es la ausencia de necesidad de instalación de paquetes
de software, configuración y dependencias. Basta con tener un navegador
y una conexión a internet, todos los archivos necesarios para la ejecución 
de la aplicación se sirven de manera automática al introducir la URL dentro de la barra de navegación.

El cliente no tiene la necesidad de instalar actualizaciones 
ya que únicamente se actualiza la versión que proporciona el servidor,
esto elimina las incompatibilidades entre versiones ya que todos los
clientes usarán la misma versión.
Desarrollo unificado, con esto hacemos referencia a que no se necesita
desarrollar para los distintos sistemas operativos (Windows, MacOS,
Linux/Ubuntu, etc.) así como conocer sus entornos gráficos y 
dependencias del sistema operativo, lo único necesario es saber HTML5, 
JavaScript y CSS3 , el navegador se encarga de interpretar los
distintos lenguajes.\\

No todo son ventajas, como desventaja sabemos que las aplicaciones 
web son algo más lentas debido a la necesidad de descargar los recursos
y no ser lenguajes compilados como C++ sino interpretados. Ésta 
desventaja cada vez va siendo menor debido a las mejoras en los interpretes de JavaScript en los
navegadores y protocolos como por ejemplo \emph{AJAX (Asynchronous 
JavaScript and XML requests} que trata de una técnica de peticiones 
ligeras para aplicaciones interactivas.

Además, con los años ha aumentado el número de navegadores distintos y
desarrollar la aplicación para todos ellos es costoso aunque existen
entornos que facilitan esta tarea como pueden ser Express y Loopback
para el lenguaje JavaScript y Django para el lenguaje Python.\\

El uso de las tecnologías web ha aumentado debido al lanzamiento del nuevo estándar HTML 5 en el que
se tienen características importantes para la transmisión de contenido multimedia con la inserción
de la etiqueta \emph{video y canvas} en las que podemos incluír vídeos y generar escenas 2D y 3D 
respectivamente. A continuación se enumeran y explican algunas de las características más destacables
del nuevo estándar HTML 5.

\begin{itemize}
	\item Aceleración gráfica con WebGL y la etiqueta \emph{canvas}. Esto permite el uso de aceleración
	gráfica hardware basada en OpenGL evitando el uso de \emph{plugins} (extensiones) y generando estos
	gráficos a partir de código JavaScript. Los elementos WebGL se pueden mezclar con otros elementos
	HTML y componerse con otras partes de la página o el fondo de la misma.
	
	\item WebWorkers, el nuevo estándar HTML 5 ofrece un API para generar ejecuciones de código en
	segundo plano en paralelo con el programa principal, esto permite operaciones por hilos de 
	ejecución con un mecanismo de mensajes entre hilos para el control de ejecución.
	
	\item WebStorage, este API es la evolución del mecanismo de las \emph{cookies} para mantenimiento de 
	estado en el protocolo HTTP el cual es sin estado. El tipo de almacenamiento es similar
	al de las cookies, se usa el par clave-valor. La principal diferencia entre el ambos mecanismos es
	que WebStorage ofrece mayor capacidad para guardar datos en el navegador y una mejor seguridad.
	
	\item WebSockets, este API permite abrir un canal de comunicación bidireccional con un servidor 
	o con una aplicación de terceros. El tipo de comunicación usada es mediante eventos, ambas partes
	de la comunicación deben manejar el mismo tipo de mensajes. Los mensajes los define el 
	desarrollador de la aplicación. El protocolo de apertura de WebSockets consiste en un inicial
	\emph{handshake} en el cual ambas partes de la comunicación se ponen de acuerdo en el canal a usar
	y entonces se inicia la transmisión de mensajes sobre TCP. Como características, destacar que al
	ir sobre el protocolo TCP se ofrece redundancia, es decir, si el mensaje se pierde se vuelve a 
	enviar ofreciendo la fiabilidad de que el mensaje llegará al destino y además son transmisiones 
	de baja latencia.
	
	\item ServerSent Events, define un API para la apertura de una conexión HTTP en la cual el servidor 
	puede enviar notificaciones al cliente, dotando de capacidad de iniciativa de envío de mensajes
	del lado servidor. Previo a HTML 5 esto no era posible, el servidor tenía que esperar una petición
	del cliente para responder.
	
	\item WebRTC, define un API de transmisión y recepción de contenido multimedia desde otro navegador
	o dispositivo que implemente los protocolos de transmisión en tiempo real. Lo más importante de esta
	API es que abstrae al desarrollador de toda la problemática que trae consigo el envío de audio y 
	vídeo como son el retardo, el \emph{jitter}, el uso del mismo formato de transmisión, etc. El \emph{jitter}
	es un problema muy importante en transmisión de audio y vídeo y es la variación del retardo 
	de la transmisión debido a la fluctuación de la red. Una aplicación que hace uso de este API es 
	\url{www.appear.in} que es un servicio de videollamada similar a Skype pero en Web. La Figura ~\ref{fig:appear-front} muestra la aplicación \emph{appear.in}.
	
	 \item WebVR, especificación web que permite generar escenas de realidad virtual en aplicaciones
	 web. Provee de un API JavaScript que da soporte a la mayoría de dispositivos de realidad
	 virtual como \emph{HTC Vive}. La intención del API es detectar los dispositivos, sus 
	 características, la frecuencia de refresco de funcionamiento y ofrecer control de posición.
	 Utiliza el motor WebGL anteriormente mencionado para el renderizado de las imágenes 3D. Un ejemplo de esta tecnología se muestra en la Figura ~\ref{fig:webvr-example}
\end{itemize}


\begin{figure}
	\centering
	\includegraphics[width=12cm, height=7cm]{img/webgl-web.PNG}
	\caption{Ejemplo de una página web que hace uso de la etiqueta canvas, webVR y aceleramiento
	gráfico con WebGL.}
	\label{fig:webvr-example}
	
	\includegraphics[width=12cm, height=7cm]{img/appear.PNG}
	\caption{Página de inicio de la web \emph{appear.in} que hace uso de tecnologías WebRTC para
	hacer videollamadas.}
	\label{fig:appear-front}
\end{figure}


Estos avances en las tecnologías web conforman lo que es conocido como web 2.0 que se centra en 
fomentar la interacción con el usuario final, el compartir información y diseño centrado en el usuario.
Gracias a HTML 5 se está experimentando un modelo de web semántica en la cual cada elemento de la red
está identificado unívocamente lo que permite enlazar la información permitiendo que el acceso a ella
sea mucho más sencillo. Lo que se intenta emular son las relaciones entre elementos de la web como si
una base de datos se tratase en el que todos los datos tienen una relación. En la figura 
~\ref{fig:web-evolution} se muestra de manera esquemática y muy resumida la evolución de la web.\\

\begin{figure}
	\centering
	\includegraphics[width=10cm, height=7cm]{img/web-comparativa.jpg}
	\caption{Nuevas relaciones que se establecen según evoluciona la web entre
	los usuarios y el contenido.}
	\label{fig:web-evolution}
\end{figure}

Un ejemplo de web semántica se encuentra en \emph{dbpedia} (\url{https://wiki.dbpedia.org/}) que se 
trata de un proyecto que toma los datos de la web \emph{Wikipedia} pero estableciendo relaciones entre
todos los elementos existentes de la wikipedia.\\

Gracias a este crecimiento de las tecnologías y estándares web comienzan a surgir numerosos entornos
de desarrollo tanto para la parte cliente como la parte servidora de un servidor web. Estos entornos
desarrollar de manera más eficiente y menos costosa nuestra aplicación web. A continuación se enumeran
algunos entornos de desarrollo más usados actualmente.

\begin{itemize}
	\item NodeJS, entorno de desarrollo de servidores en lenguaje JavaScript. Provee de las 
	herramientas básicas para el manejo de peticiones del protocolo HTTP.
	
	\item Django, entorno de desarrollo de servidores en lenguaje Python, implementa el MVC 
	(Modelo Vista Controlador) que simplifica mucho la creación de paginas web gracias a su motor de 
	plantillas.
	
	\item React, entorno de desarrollo de interfaces de usuario, permite desacoplar el lado cliente
	del lado servidor y programación orientada a componentes.
	
	\item Loopback, entorno de desarrollo de servidor en lenguaje JavaScript, se construye
	encima de NodeJS y se utiliza para acelerar el desarrollo de API Rest.
\end{itemize}


Como conclusión, el desarrollo de las tecnologías web está experimentando actualmente un crecimiento
muy grande. Cada vez las aplicaciones web son capaces de integrar más capacidades y de manera más
eficiente lo que hace que cada vez sean más los desarrolladores de software que adoptan este 
modelo de desarrollo. 



\section{Estructura de la memoria}
\label{sec:estructura}

En esta sección se detalla la estructura de la memoria que constar de las siguientes partes:

\begin{itemize}
  \item El capítulo~\ref{chap:intro} es una introducción al campo en el que se desarrolla el proyecto,
  se explica la motivación del proyecto.

  \item En el capítulo~\ref{chap:cap-objetivos} se muestran los objetivos a 
  conseguir con la elaboración del proyecto y la estructura de la hoja de ruta.

  \item En el capítulo~\ref{chap:herramientas} se presentan las 
  tecnológicas que se han utilizado para el desarrollo del proyecto y se 
  explica por que se han elegido dichas tecnologías y no otras.

  \item En el capítulo~\ref{chap:disenno} se explicarán el diseño del simulador, soporte del robot, es decir, qué funcionalidades tiene y 
  la conectividad de que dispone.
  
  \item En el capítulo~\ref{chap:robotica-educativa} se muestran dos aplicaciones docentes que usan el simulador desarrollado y los ejercicios a resolver en ambas aplicaciones.
  
  \item Finalmente en el capítulo~\ref{chap:conclusiones} se hace una 
  valoración de todo lo que ha conllevado el proyecto y se proponen 
  futuras implementaciones o mejoras de WebSim.
\end{itemize}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% OBJETIVOS %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\cleardoublepage % empezamos en página impar
\chapter{Objetivos y planificación} % título del capítulo (se muestra)
\label{chap:cap-objetivos} % identificador del capítulo (no se muestra, es para poder referenciarlo)

La planificación y consecución de objetivos es un pilar fundamental en el desarrollo de software, cabe mencionar la metodología y objetivos fundamentales del presente proyecto para dar a conocer el flujo de trabajo.

\section{Objetivos} % título de sección (se muestra)
\label{sec:objetivo} % identificador de sección (no se muestra, es para poder referenciarla)

El objetivo general de este trabajo fin de grado consiste en crear una herramienta educativa (WebSim) con 
simulación de robots para la plataforma Kibotics en la cual el peso computacional la 
lleve el lado cliente en lugar del lado servidor.

Este objetivo general se ha articulado en tres subojetivos. Primero un simulador basado en tecnologías web que han de soportar la infraestructura hardware utilizada actualmente por JdeRobot-Kids,
el PiBot, que está formado por los siguientes sensores y actuadores:

\begin{itemize}
	\item \emph{Motores}: dos servomotores independientes que 
	dotan de movimiento al robot.
	
	\item \emph{Cámara}: una minicámara, esto le da funcionalidad muy 
	importante al robot como puede ser la detección visual de obstáculos.
	
	\item \emph{Sensores IR}: dos sensores infrarrojos posicionados 
	en la parte baja del chasis del robot, permiten la 
	detección de colores, objetos y formas.
	
	\item \emph{Sensor de ultrasonidos}: Dos sensores de ultrasonido 
	posicionados en la parte delantera del chasis del robot, su 
	funcionalidad es la de sensores de proximidad lo que permite saber 
	no solo si hay un objeto delante sino que también
	a qué distancia está dicho objeto.
	
\end{itemize}

Además existen dos subobjetivos más, la creación de dos aplicaciones web para el uso de WebSim a través
de sendos editores de código distintos en los cuales los alumnos que usen el simulador podrán resolver
una serie de ejercicios educativos propuestos por Kibotics. Las aplicaciones permiten crear programas robóticos en distintos lenguajes, la primera en \emph{JavaScript} y la sengunda en lenguaje visual de bloques, \emph{Blockly}.


\section{Metodología}
\label{sec:metodologia}

Para el desarrollo del proyecto se ha seguido la metodología \emph{Agile}, que es una forma de realizar
los proyectos en la cual el proyecto al completo del software se parte en partes más pequeñas que se desarrollan 
en un par de semanas. De modo que el cliente puede ir haciendo pequeños cambios al proyecto en función
de la ventaja de mercado que quiera obtener y le permite ir viendo el desarrollo del proyecto en
intervalos de tiempo reducidos. Estos intervalos de tiempo se llaman \emph{sprints} en el cual el 
desarrollador se centra únicamente en programar el software y si el \emph{sprint} lo permite se 
genera documentación de lo hecho en el \emph{sprint}.\\

Para la simulación de esta metodología se han hecho reuniones semanales con los tutores del TFG en el
cual se les presentaba un prototipo y se proponían cambios, mejoras y desarrollos a llevar a cabo
en la siguiente semana. Por norma los \emph{sprints} se sobredimensionaban, es decir, se proponían 
desarrollos que no iban a entrar en la planificación temporal, ésto se ha hecho así para mantener
la idea general del proyecto y no desviarnos de esta idea y además evitar el problema de no 
desarrollar nada más si lo fijado en la reunión ya se había completado.


\section{Planificación temporal}
\label{sec:planificacion_temporal}

Como se ha comentado se ha seguido una metodología en \emph{sprints}, a 
continuación se muestran dos imágenes con los intervalos temporales cada uno de los
\emph{sprints}.

\begin{figure}[h]

  \includegraphics[width=\linewidth, height=5cm]{img/GANTT-1.PNG}
  \caption{Flujo de trabajo en hitos para los sprints 1 al 4}
\label{fig:flujo-1}
\end{figure}
\begin{figure}[h]
	\includegraphics[width=\linewidth, height=5cm]{img/GANTT-2.PNG}
  \caption{Flujo de trabajo en hitos para los sprints 5 al 8.}
\label{fig:flujo-2}
\end{figure}

Como se ve en las Figuras ~\ref{fig:flujo-1} y ~\ref{fig:flujo-2} algunos de los \emph{sprints} han ocupado más de una reunión semanal. En
esas reuniones se presentaba el estado actual del proyecto aunque no estuviesen finalizados los 
hitos marcados del \emph{sprint}.


El nivel de esfuerzo para este proyecto ha sido alto debido a la 
necesidad de aprender diferentes tecnologías como son A-FRAME, jQuery,
OpenCVjs, WebPack y NPM. Se dedicaban alrededor de 4-5 horas al día cada día de la 
semana a excepción de los fines de semana que se añadían 2 horas más 
al anterior intervalo.


\section{Control de versiones}
\label{sec:versiones}

Como en todo desarrollo de software es necesario el uso de una plataforma de control de versiones.
Para este proyecto se ha utilizado \emph{GitHub}. Github es 
una plataforma que permite crear varias versiones y volver a una versión anterior si la actual deja
de funcionar. Permite crear ramas distintas con funcionalidades de código distintas lo que permite
hacer pruebas de una funcionalidad sin 'estropear' el código ya existente. Además permite tener tu
código actualizado en cualquier ordenador. \\

Github tiene una extensa comunidad de desarrolladores, los repositorios son de uso libre, es decir, 
cualquiera que use Github puede clonar el código de tu repositorio para usarlo, modificarlo e incluso
contribuir en él ayudando a solventar errores o descubriendo nuevas incidencias. Todas estas 
características lo hacen muy potente y es una de las más usadas por la comunidad del desarrollo de software libre en general.\\

Como característica interesante Github permite visualizar de forma gráfica la
actividad que tiene un repositorio, lo que permite indicar qué repositorios están muy activos o tienen
una gran comunidad de desarrolladores que le dan soporte y cuáles están en desuso. \\

\begin{figure}[h]
	\centering
	\includegraphics[width=15cm, height=7cm]{img/github-commits.PNG}
	\caption{Gráfico de actividad del repositorio de \emph{A-FRAME} en Github.}
	\label{fig:github-commits}
\end{figure}


Se ha trabajado en dos repositorios distintos 
\footnote{\url{https://github.com/JdeRobot/WebSim}}
\footnote{\url{https://github.com/RoboticsURJC-students/2018-tfg-alvaro_paniagua}}.

En el primer repositorio la metodología de trabajo era mediante incidencias en las cuales se marcaba
una incidencia importante, se creaba una rama para arreglar la incidencia y se subía un 
parche (\emph{pull request}) a modo de parche para resolver la incidencia \emph{issue} lo que permite que el desarrollo quede muy 
limpio. Esta metodología es útil en repositorios en los cuales hay varios desarrolladores trabajando
en partes distintas del software como era el caso.

En el segundo repositorio al haber un único desarrollador se ha trabajado únicamente con 
\emph{commits}. 



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% ESTADO DEL ARTE (FRAMEWORKS) %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Herramientas}
\label{chap:herramientas}


En este capítulo se desarrollarán de manera breve las herramientas utilizadas para el desarrollo del
proyecto. Han sido elegidas debido a que eran las que mejor se ajustaban a las 
necesidades del proyecto o bien otras como HTML, y JavaScript debido a que son los
estándares de desarrollo para aplicaciones web.


\section{Lenguaje JavaScript}
\label{sec:javascript}

\emph{\emph{JavaScript}} fue creado por Brendan Eich en 1995 cuando
trabajaba para Netscape Communications inspirado por el lenguaje Java. Se encuentra 
actualmente bajo el estándar \emph{ES6 (ECMAScript 6 o ECMAScript 2015} que añade a la versión anterior del lenguaje ciertas
características.

Es el lenguaje más utilizado para el desarrollo Web, permite que las aplicaciones web 
sean interactivas, es decir, permite hacer actualizaciones de contenido en el momento, 
mostrar mapas, animaciones 3D. Es el tercer pilar del estándar de tecnologías web, 
compuesto por \emph{\emph{HTML, CSS y JS}}.

\subsection{Características del lenguaje}
\label{subsec:JS-features}

\emph{\emph{JavaScript}} es un lenguaje de \emph{scripting} orientado al lado cliente de una
aplicación web como ya se ha comentado anteriormente, y cuenta con las siguientes características:

\begin{itemize}
	\item Lenguaje de \emph{alto nivel}, está lejos de la implementación en un ordenador y la arquitectura \emph{hardware} subyacente, se centra en aspectos lógicos implementados con un lenguaje más accesible al usuario. Un ejemplo de una función en JavaScript se muestra a continuación.
	\begin{lstlisting}
	function myFunction(){
		console.log("Hello world");	
	}
	\end{lstlisting}
	
	\item Lenguaje basado en \emph{objetos}, una estructura
	habitual en programación que se refiere a la encapsulación de 
	operaciones y estados en un modelo de datos. Otros lenguajes 
	orientados a objetos son \emph{Python, Ruby, Java, etc.} 
	
	\item Tipado débil, no es necesario declarar el tipo de una variable. 
	Una variable cualquiera puede ser de distintos tipos en momentos distintos pasando de un 
	\emph{string} a un \emph{number} o a un \emph{objeto}. En la figura ~\ref{fig:js-code} se 
	muestra un ejemplo de esta característica en el que la variable \emph{msg} es declarada
	inicialmente con contenido de tipo \emph{string} y posteriormente se modifica su contenido para
	que sea de tipo \emph{number}.
	\begin{figure}[h]
		\centering
		\includegraphics[width=13cm, height=3.5cm]
		{img/js-vars.PNG}
		
		\caption{En la parte izquierda  se muestra el 
		código que imprime la variable \emph{msg} tras modificar su tipo}
		
		\label{fig:js-code}
	\end{figure}
	
	\item Es un lenguaje \emph{case-sensitive},  distingue las letras mayúsculas de las 
	minúsculas, a la hora de declarar una variable o función no es lo mismo \emph{miVariable} que
	\emph{mivariable}. La manera recomendada para definir nombres de variables o funciones en este lenguaje
	es seguir la sintaxis \emph{camel-case} en la cual si el nombre de mi variable está compuesta por
	varias palabras la primera de ellas estará completamente en minúsculas y las palabras que la siguen
	tendrán la primera letra en mayúsculas, a continuación se muestra un ejemplo: 
	\emph{miVariableDeMuestra}.
	
	\item Al igual que otros lenguajes como \emph{Python}, \emph{JavaScript} es un lenguaje 
	\emph{interpretado} esto quiere decir que no se necesita un compilador para crear un binario del
	código sino que existe un intérprete dentro del navegador que se encarga de ejecutarlo.
\end{itemize}


\emph{JavaScript} se encuentra bajo el estándar
\emph{ES6} lo cual le dota de una serie de características importantes, a continuación se citarán
algunas de las que son más relevantes para el proyecto:

\begin{itemize}
	\item Definición de variables con ámbito local \emph{(Block-Scoped Variables)}, esta característica
	permite declarar una variable que únicamente existirá en un determinado bloque y no fuera de éste.
	Mejora la inteligibilidad del código a la hora de que lo tengan que leer distintos 
	desarrolladores de un equipo. Esta característica existía anteriormente pero el desarrollador 
	debía tener conocimiento del \emph{scoping} de variables, es decir, cuándo una variable afectaba 
	a un bloque y cuándo no. En la figura ~\ref{fig:let-scope} se muestra un uso de este tipo
	de declaración, la variable \emph{x} únicamente existe dentro del bucle \emph{for}.
	\begin{figure}[h]
		\centering
		\includegraphics[width=7cm, height=1.7cm]
		{img/let-scope.PNG}
		
		\caption{Ejemplo de la declaración de variables locales con la etiqueta \emph{let}.}
		
		\label{fig:let-scope}
	\end{figure}
	
	\item Funciones de flecha, en el estándar ES6 se permite la declaración de funciones sin la creación
	de un contexto \emph{this}, este contexto hace referencia al bloque en el que se encuentra.
	Anteriormente a ES6 sí se necesitaba usar el contexto de otra función en un determinado bloque
	había que hacer la siguiente transformación \emph{var self = this;} lo que permitía usar el 
	contexto dentro de una nueva función llamando a \emph{self}. Gracias a ES6 esto ha cambiado, la 
	figura ~\ref{fig:arrow-func} muestra un ejemplo de la nueva sintaxis.
	\begin{figure}[h]
		\centering
		\includegraphics[width=6cm, height=1.7cm]
		{img/arrow-func.PNG}
		
		\caption{El contexto \emph{this} es siempre el mismo gracias a la declaración de la función
		de flecha.}
		
		\label{fig:arrow-func}
	\end{figure}
	
	\item Exportar e importar módulos. Esta característica es similar a los import de \emph{Python} lo
	que dota de mucha modularidad a la aplicación ya que permite tener un programa principal que 
	controla el uso de las distintas funcionalidades evitando así las aplicaciones monolíticas y permite
	que cada nueva funcionalidad sea completamente independiente de las demás lo que hace que la 
	modificación de esta funcionalidad no afecte a las demás. La figura ~\ref{fig:js-export} muestra
	la sintaxis para exportar una variable y una función para ser utilizada en distintos scripts.
	
	\begin{figure}[h]
		\centering
		\includegraphics[width=11.5cm, height=5.5cm]
		{img/js-exports.PNG}
		
		\caption{Sintaxis para importar o exportar funciones entre distintos 
		programas dentro de una aplicación o entre varias permite reutilizar mismo código en
		distintas aplicaciones.}
		
		\label{fig:js-export}
	\end{figure}	
	
	\item Declaración de clases. Es una modificación que permite mejorar la inteligibilidad del código
	ya que esta característica se permitía en anteriores estándares pero con una sintaxis distinta
	que podría dar lugar a confusiones a la hora de entender el código. La figura ~\ref{fig:class-def}
	muestra una comparativa entre ambas sintaxis, en la parte superior vemos la sintaxis en estándares
	anteriores y en la parte inferior la sintaxis de ES6.
	
	\begin{figure}[h]
  		\centering
  		\begin{minipage}[b]{0.4\textwidth}
    		\includegraphics[height=3.5cm, width=8cm]{img/class-prev.PNG}
  		\end{minipage}
  		\hfill
  		\begin{minipage}[b]{0.4\textwidth}
    		\includegraphics[height=3.7cm, width=5cm]{img/class-es6.PNG}
  		\end{minipage}
  		\caption{Comparativa de sintaxis de declaración de un objeto antes y después de ES6.}
  		\label{fig:class-def}
	\end{figure}

\end{itemize}

\subsection{Entornos de desarrollo JavaScript}
\label{subsec:js-frameworks}

Este lenguaje tiene mucho peso en el lado cliente web, es decir, en el código que se 
ejecuta en el navegador, pero gracias a los avances anteriormente mencionados y a la llegada del 
entorno \emph{NodeJS} este lenguaje ha comenzado a tomar más importancia en el lado servidor debido a
que ya no existe la necesidad de aprender dos lenguajes (javascript en cliente, ruby, python, java en
el servidor) sino que comienzan a surgir los desarrolladores \emph{fullstack JavaScript}.
Debido a éste 'boom' del lenguaje se comienzan a crear distintos entornos de desarrollo que tratan de
cubrir distintas problemáticas en el desarrollo web.

Actualmente \emph{JavaScript} se utiliza en multitud de entornos tanto de lado 
cliente como en el lado servidor, a continuación se enumeran algunos de los más usados:

\begin{itemize}
	\item \emph{NodeJS}: Tecnología en el lado servidor con código completamente 
	JavaScript asíncrono y orientado a eventos. NodeJS fue diseñado para construir 
	aplicaciones en red escalables. Su principal característica es la capacidad de 
	gestionar multitud de conexiones simultáneas de una manera muy eficaz gracias a su
	arquitectura. Se basa en un hilo que escucha peticiones y las redirecciona a 
	\emph{hebras} distintas, cada una de estas hebras ejecuta el código necesario y
	una vez la hebra termina lanza un evento al hilo de peticiones indicando que ha 
	terminado de ejecutar la tarea indicada, entonces es el hilo de peticiones el que se
	encarga de devolver la respuesta.
	
	\item \emph{Express y Loopback}: Ambos entornos son extensiones de NodeJS y 
	permiten la creación de un API en el lado servidor. Loopback es actualmente más 
	importante que Express ya que ofrece muchas herramientas para la creación de
	API Rest de manera muy sencilla así como conectores a la mayoría de bases de datos
	como puede ser MongoDB, mySQL, sqlite3, etc.
	
	\item \emph{AngularJS} Es un entorno basado en el \emph{Modelo Vista Controlador}
	para el desarrollo en la parte cliente que permite la creación de aplicaciones web
	\emph{SPA} (\emph{Simple-Page-Application}). AngularJS permite extender el 
	lenguaje HTML con directivas y atributos sin perder la semántica.
	
	\item \emph{A-FRAME} Es un entorno de creación de escenas de 
	\emph{Realidad Virtual}, se hablará de él de manera más extensa en el apartado ~
	\ref{sec:A-FRAME}. No es un entorno muy extendido debido a su juventud pero
	cuenta con una gran comunidad de desarrolladores y es el pilar central 
	de este Trabajo Fin de Grado.
\end{itemize}

Esta presentación de los entornos disponibles para JavaScript sirve como 
respaldo ante la decisión de elegir el lenguaje. Además, como se ha comentado en el capítulo
~\ref{chap:intro} el proyecto se orienta a la creación de una 
aplicación con peso en el lado cliente, por tanto sabiendo ésto y teniendo en cuenta que
A-FRAME está creado en el lenguaje JavaScript la elección de este lenguaje adquiere importancia por 
sí misma.


\section{Lenguaje HTML}
\label{sec:html}

\emph{\emph{HTML}} fue creado por \emph{Tim Berners-Lee} en 1990 y es el acrónimo para
\emph{HyperText Markup Language} (Lenguaje de marcas de hipertexto). Se utiliza para la creación de documentos electrónicos que se envían a través de la red
global (internet). Cada documento tiene una serie de conexiones a otros documentos 
llamados \emph{hyperlinks} que permiten la navegación entre distintos recursos.

\emph{HTML} asegura el formato correcto de texto, imágenes y estilos para poder leer
un documento en el navegador con la forma original con la que se generó el documento. En
la figura ~\ref{fig:estructura-html} se muestra una página HTML muy simple y se explican 
sus distintas partes y su función.

\begin{figure}[h]
	\centering
	\includegraphics[width=9.5cm, height=4.5cm]{img/html-ejemplo.PNG}
	\caption{Estructura de una pagina HTML simple con un título y un párrafo}
	
	\label{fig:estructura-html}
\end{figure}

Un documento HTML tiene una estructura de árbol donde
la etiqueta \emph{html} es el elemento raíz y cada nuevo elemento es una rama del anterior.
Estas 'ramas' se pueden ir extendiendo según la necesidad del proyecto web.

Como elementos principales del documento HTML tenemos la declaración documento 
\emph{$<$DOCTYPE html$>$} que en este caso indica que estamos ante un documento
HTML 5 que es última versión de HTML. La 
etiqueta \emph{html} marca la raíz del documento, dentro de ésta etiqueta tenemos dos 
etiquetas importantes:

\begin{itemize}
	\item \emph{HEAD} es la cabecera del documento, contiene los metadatos del documento
	como el título, la codificación de caracteres utilizada y enlaces a otros recursos 
	adicionales como pueden ser \emph{scripts} y \emph{hojas de estilos}.
	
	\item \emph{BODY} es el contenido que se mostrará del documento, puede contener
	imágenes, enlaces a otros documentos, vídeos, menús de navegación, formularios, 
	botones e incluso escenas animadas como la que se mostrarán en el presente proyecto.
\end{itemize}

Las combinaciones de elementos son muy amplias, no existe una única estructura válida para
un documento HTML sino que se genera una estructura en función de la aplicación.

A continuación se enumeran 
las nuevas características del estándar HTML5 que tienen relación con el proyecto:

\begin{itemize}
	\item \emph{VIDEO}, esta etiqueta es una de las nuevas características
	de HTML5 y una de las más importantes, permite embeber vídeos dentro de una página 
	web de manera nativa sin el uso de \emph{plugins}.
	
	\item \emph{NAV}, esta etiqueta declara un elemento de tipo \emph{barra de
	navegación} en el cual se encuentra el menú con enlaces a otros tipos de recursos
	y secciones tanto dentro como fuera de la página. En nuestro caso esta etiqueta se ha
	utilizado para encapsular los botones de arranque/pare del código creado por el alumno
	y el botón que permite mostrar u ocultar la cámara del robot.
	
	\item \emph{CANVAS}, permite la renderización de escenas gráficas a través de 
	JavaScript. Es la etiqueta más importante dentro de nuestro proyecto ya que es la
	etiqueta que nos permite la creación de la escena en la cual tenemos nuestro 
	robot simulado.
\end{itemize}

Existe una característica importante que afecta a todo el documento de HTML5 pero que,
en general, no se está respetando en el desarrollo web y es que HTML5 permite una organización
semántica, es decir, las etiquetas como \emph{SECTION, NAV y FOOTER} marcan claramente
zonas dentro del documento HTML con el fin de poder conocer la estructura del documento
de manera clara.



\section{Hojas de estilo CSS}
\label{sec:css}

\emph{\emph{CSS}} o \emph{Cascading StyleSheet} es un lenguaje que se usa para definir
el aspecto visual de una página HTML. Su principal misión es la de separar la estructura
y contenido del aspecto de la pagina HTML.\\

Con \emph{CSS} podemos controlar incluso cómo se van a ver todos los documentos HTML de
la aplicación. Es comúnmente utilizado por las empresas y diseñadores gráficos para crear
de manera visual una identificación de la aplicación mediante tipos de letra y paleta de
colores utilizada.

Deja atrás el uso de JavaScript para fines de representación visual lo 
que hace que el rendimiento de la página se mejore al usar código JavaScript para otros 
fines. Además reduce la dependencia de software de edición gráfica como \emph{Photoshop},
que sigue siendo utilizado pero su función es la edición más avanzada.

A continuación se muestra una imagen con una comparativa de la misma página web con y 
sin CSS.

\begin{figure}[h]
	\centering
	\includegraphics[width=9cm, height=7cm]{img/css-comparacion.PNG}
	\caption{Imagen comparativa de la misma pagina web con y sin hojas CSS}
	
	\label{fig:css}
\end{figure}

Como vemos en la figura ~\ref{fig:css} las hojas de estilo CSS permiten modificar
completamente el aspecto de la página así como esconder menús y delimitar de manera visual 
las distintas partes de la página, lo que permite mejorar la experiencia de usuario
haciendo más accesibles las partes importantes de la página.


La mención a la interfaz de usuario es importante ya que es uno de los puntos
que más se cuidan en las empresas e incluso se hacen estudios para mejorar las interfaces
. Por ejemplo, debido al tamaño de los nuevos \emph{smartphones} el menú de navegación
ha cambiado su posición debido a que era difícil alcanzar la parte superior de la pantalla
y por tanto se hacía molesto el uso de la aplicación.


\section{A-FRAME}
\label{sec:A-FRAME}

\emph{\emph{A-FRAME}} es un entorno web para la construcción de escenas de realidad
virtual, se creó con la intención de facilitar la creación de contenido de realidad 
virtual. Es un entorno de código libre y tiene una de las comunidades de creadores de
realidad virtual más grandes actualmente.

Soporta la mayoría de gafas de realidad virtual como Vive, Rift, GearVR, etc.
Además se puede usar no solo para realidad virtual sino para realidad aumentada. A-FRAME 
fomenta la creación de escenas inmersivas completas de realidad virtual y va más allá
de únicamente generar contenido en 360º, también implementa el uso de control de posición 
y controladores (mandos) que permiten interactuar con la escena, estos controles permiten
al usuario tener una experiencia más inmersiva en la escena.

A-FRAME está soportado en los siguientes escenarios:
\begin{itemize}
	\item Realidad virtual en aplicaciones de escritorio con \emph{gadgets}.
	\item Realidad virtual en aplicaciones móviles con \emph{gadgets}.
	\item Aplicaciones de escritorio convencionales.
	\item Aplicaciones de móvil convencionales.
\end{itemize}


A continuación se explican en las siguientes subsecciones las características más importantes del 
entorno A-FRAME.

\subsection{Primitivas y HTML}
\label{subsec:A-FRAME_primitivas}

\emph{A-FRAME} está basado en HTML y el DOM \emph{(Document Object Model)}. HTML es un lenguaje
sencillo de leer y conocer la estructura de un documento, además no requiere de instalaciones únicamente se compone
de texto y un navegador que muestre la página. A-FRAME es compatible con la mayoría de entornos
que se utilizan actualmente en el desarrollo web como pueden ser Vue.js, React, AngularJS y jQuery.

A-Frame permite crear escenas de realidad virtual de manera muy simple, como se ve en la figura~\ref{fig:A-FRAME-scene}
únicamente se necesita una etiqueta \emph{script} que haga referencia al código del entorno y una
etiqueta \emph{a-scene} dentro del cuerpo del documento para crear una simple escena. 

A-FRAME ofrece un conjunto de elementos básicos para la escena llamados primitivas, estos elementos
son figuras básicas como \emph{cajas, esferas, cilindros, planos, cielo, etc.} A-FRAME no solo ofrece
figuras básicas, su intención es crear escenas inmersivas
completas, por ello ofrece además etiquetas para la inyección de sonidos y vídeos dentro de la escena.

Este tipo de primitivas son bastante útiles para su uso en escenas simples, todas ellas heredan 
de la primitiva \emph{a-entity} que, equiparándolo con HTML convencional, equivaldría con la
etiqueta \emph{div} del estándar HTML, la cual se utiliza de muchas maneras distintas.
Esta etiqueta \emph{a-entity} representa por tanto el punto de partida de cualquier tipo de elemento
de la escena que queramos crear al cual se le irán añadiendo \emph{componentes} que le dotarán de
cierta funcionalidad específica.

A-FRAME permite además crear nuestras propias primitivas lo que hace posible seguir el principio de 
programación \emph{DRY (Don't Repeat Yourself)} por el cual si tenemos un complicado elemento en la
escena que está compuesto de varias entidades distintas no tenemos que copiar ese código \emph{N} veces
sino que podemos registrar la primitiva y hacer referencia a este elemento mediante el nombre de 
etiqueta que más convenga.

La figura ~\ref{fig:A-FRAME-scene} muestra todo el código necesario para crear la escena mostrada.
\begin{figure}[h]
	\centering
	\includegraphics[width=13cm, height=4cm]{img/html-aframe.PNG}
	\includegraphics[width=13cm, height=6cm]{img/aframe-scene.PNG}
	\caption{Ejemplo de código HTML que renderiza una escena básica de realidad 
	virtual}
	\label{fig:A-FRAME-scene}
\end{figure}
	
	
	
\subsection{Sistema Entidad-Componente}
\label{subsec:entidad_componente}
A-FRAME se basa en el entorno \emph{\emph{three.js}} y provee una estructura reutilizable de entidad-
componente en la que un componente puede ser utilizado en distintas entidades de distinta clase.
Se pueden generar componentes personalizados y vincularlos a cualquier tipo de 
entidad dándole una funcionalidad distinta. Esto permite una gran flexibilidad a la 
hora de generar distintos integrantes en la escena con funcionalidades diferentes
pero heredando todos de una misma entidad.

La arquitectura entidad-componente es común en el desarrollo 3D y en el desarrollo de videojuegos y 
sigue el principio de composición por herencia. Los beneficios de este tipo de arquitectura son:

\begin{itemize}
	\item Gran flexibilidad a la hora de crear objetos debido a la reutilización de componentes y 
	la mezcla de distintos componentes.
	\item Elimina el problema de largas cadenas de herencia, cada componente es independiente.
	\item Diseño limpio gracias al desarrollo por módulos.
	\item Es la manera más escalable de generar complejas escenas de realidad virtual.
	\item Permite reutilizar y compartir componentes no solo en un mismo proyecto sino con la comunidad
	de desarrolladores.
\end{itemize}

En la figura ~\ref{fig:ecs} se muestra un esquema del \emph{Sistema Entidad-Componente} en el cual
tenemos una figura final con forma de caja la cual estaría compuesta de varios componentes distintos. 
Estos componentes son: \emph{Posición, Geometría, Material y Color}.

\begin{figure}
\centering
	\includegraphics[width=15cm, height=3cm]{img/ecs.PNG}
	\caption{Etiquetas que serían el equivalente en A-FRAME a un componente,
	el conjunto de varios componentes dan forma a la caja}
	\label{fig:ecs}
\end{figure}

A continuación se mostrará la API \emph{(Application Program Interface)} que ofrece A-FRAME para 
implementar el \emph{Sistema Entidad-Componente}:
\begin{itemize}
	\item \emph{Entidad:} se representa en A-FRAME mediante la etiqueta \emph{a-entity}.
	\item \emph{Componentes:} se representa en A-FRAME como atributos de la etiqueta HTML. Estos 
	componentes son objetos que contienen un esquema, manejadores y métodos. Éstos se registran mediante
	el método \emph{A-FRAME.registerComponent(nombre, definición);}. A continuación se muestra un 
	componente que imprime un mensaje en la consola del navegador.
	\begin{figure}[h]
		\centering
		\includegraphics[width=12cm, height=3cm]{img/componente-aframe.PNG}
		\caption{Componente básico que imprime por la consola del navegador el mensaje que se le pase
		por \emph{message}}		
		\label{fig:componente}
	\end{figure}
	
	\item \emph{Sistema:} representado por la escena mediante la etiqueta \emph{a-scene}. Los 
	sistemas son similares a los componentes a la hora de definirlos, se registran mediante
	\emph{A-FRAME.registerSystem(nombre, definición)}.
\end{itemize}

Además A-FRAME tiene dos tipos de implementaciones que le dan características 
adicionales al sistema entidad-componente y es que al implementarse sobre HTML y JavaScript tenemos
dos características importantes:

\begin{itemize}
	\item Referenciar una entidad mediante el método \emph{querySelector} implementado en JavaScript
	lo que permite acceder a una entidad por su ID, clase o atributos. En la figura ~\ref{fig:querySel}
	se muestra un ejemplo de código JavaScript que accede a un elemento de A-FRAME con ID
	\emph{rightHand}.
	\begin{figure}[h]
		\centering
		\includegraphics[width=13cm, height=0.5cm]{img/querySelector-aframe.PNG}
		\caption{Ejemplo de acceso a un elemento de A-FRAME, se hace de la misma
		manera que se accede a cualquier elemento de HTML convencional}
		\label{fig:querySel}
	\end{figure}
	
	\item Comunicación entre las entidades mediante eventos, esta característica es hereditaria del 
	lenguaje utilizado lo que permite registrar y suscribir eventos que permite que los elementos en
	la escena no se conozcan entre sí.
	
	\item Crear, eliminar y modificar atributos mediante el API del DOM, podemos utilizar los métodos
	\emph{.setAttribute, .removeAttribute, .createElement y .removeChild} para modificar los 
	elementos.
	
\end{itemize}

Por último pero no menos importante, los componentes pueden hacer cualquier cosa, tienen acceso completo
a \emph{three.js, JavaScript y APIs Web} como pueden ser \emph{WebRTC, AJAX, etc.}

\subsection{Modelos 3-D}
\label{subsec:modelos-3d}

\emph{A-FRAME} ofrece la posibilidad de cargar modelos 3D más sofisticados en los formatos 
\emph{glTF, OBJ, COLLADA}. Se recomienda el uso del formato \emph{glTF} ya que es el modelo estándar
para transmitir modelos 3D en la WEB. Los componentes se pueden escribir de manera que se pueda manejar
cualquier tipo de formato que tenga un objeto en \emph{three.js} para cargar el modelo.

Los modelos son archivos en texto plano y contiene vértices, caras, texturas, materiales y animaciones.

Como se ha repetido en varias ocasiones se trata de crear escenas, para ello es necesario implementar 
animaciones. Estas animaciones se implementan con el paquete de componentes creado por
\emph{Don McCurdy}, se puede localizar en la web \footnote{\url{https://github.com/donmccurdy/A-FRAME-extras/blob/master/src/loaders/animation-mixer.js}}.

\subsection{Herramientas de Desarrollo}
\label{subsec:devtools}

\emph{A-FRAME} se construye sobre JavaScript y HTML por tanto utiliza las mismas
herramientas de desarrollo ya disponibles dentro del navegador. Además al crear escenas 3D se hace 
complicado depurar la escena y saber que todo está siendo representado en su po,sición correcta. Para 
esta problemática A-FRAME incluye un inspector visual que permite conocer la posición y los valores de 
los atributos para cada entidad de la escena. La figura ~\ref{fig:A-FRAME-scene} muestra la escena de
nuestro simulador y los atributos de la cámara incluida dentro del robot. Como se ve, el inspector
muestra el ángulo de visión de la cámara del robot y muestra los ejes de la escena respectivo al punto
en el que se encuentra de la cámara.\\

El inspector ofrece una representación en árbol de la escena siguiendo la estructura del documento HTML, y
ofrece la posibilidad de mover, rotar, añadir y borrar elementos de la escena así como copiar la 
etiqueta una vez movida para usarla en nuestro documento HTML. Un ejemplo de esto sería mover nuestro
robot y orientarlo de cara a la pelota verde. Sin el inspector tendríamos que ir haciendo pruebas 
modificando manualmente el atributo de la posición dentro del documento HTML pero con el inspector 
visual basta usar las herramientas para mover el elemento y copiar las coordenadas que aparecen
en la ventana de la derecha.

Por último, el inspector permite hacer capturas de movimiento lo que permite:
\begin{itemize}
	\item Test más rápidos, no se necesitan utilizar los \emph{gadgets} cada vez que se quiera hacer 
	un test lo que acelera el desarrollo de la aplicación.
	
	\item Múltiples desarrolladores pueden utilizar el mismo \emph{gadget}, puedes grabar el movimiento
	y dejar de usar el \emph{gadget} para que otros desarrolladores del mismo proyecto puedan usarlo.
	
	\item Mostrar errores del código.	
	
\end{itemize}

\begin{figure}[h]
	\centering
	\includegraphics[width=13.4cm, height=6.3cm]{img/aframe-inspector.PNG}
	\caption{Inspector visual que ofrece el entorno \emph{A-FRAME}}
\end{figure}



\section{Editor ACE}
\label{sec:ace_editor}

\emph{\emph{Editor ACE}} es un editor de código embebido creado en \emph{JavaScript}. Implementa las 
características de los editores nativos como \emph{Sublime Text, Vim, etc.} \emph{ACE} es el editor
usado en el servicio \emph{AWS Cloud9 IDE}.

Este editor ofrece la siguiente funcionalidad:
\begin{itemize}
	\item Mantener el estado de la sesión como por ejemplo el \emph{scroll}, selección de texto, etc.
	\item Resaltado de sintaxis para la mayoría de lenguajes de programación como \emph{JavaScript, CSS
	, Python, Java, etc.}
	\item Permite crear tus propias reglas de resaltado.
	\item Indentación automática del código.
	\item Manejo de ficheros grandes, maneja miles de líneas sin problema.
	\item Resaltado de paréntesis.
	\item \emph{Drag and Drop} de texto dentro del editor de código.
	\item Comprobación de sintaxis del lenguaje, esta característica es bastante útil ya que nos permite
	descartar errores en ejecución debido a la sintaxis del programa lo que acelera el desarrollo.
\end{itemize}

La característica más importante para nuestro proyecto es sin duda la facilidad para embeberlo dentro
de nuestra aplicación web, simplemente hace falta una etiqueta \emph{script} y un pequeño código para
configurar su carga en la página, el detalle del código se mostrará en el capítulo \ref{chap:robotica-educativa}.
El editor además provee de una sencilla \emph{API} para obtener el código escrito en él
a través de nuestro programa para poder manejarlo.

En la figura ~\ref{fig:ace-conf} se muestra las posibles configuraciones que puede adoptar el editor.

\begin{figure}[h]
	\centering
	\includegraphics[width=12cm, height=6cm]{img/ace-config.PNG}
	\caption{Posibles parámetros de configuración para el editor}
	\label{fig:ace-conf}
\end{figure}


El editor utiliza el DOM \emph{(Document Object Model)} para el renderizado, concretamente la etiqueta
\emph{canvas} y no depende de librerías externas. La característica más destacable del editor es que 
no es necesario instalar nada tu ordenador, el editor reside completamente en la página web lo que 
permite la creación de aplicaciones interactivas como la del presente proyecto en la cual podemos 
programar el robot 'en vivo' sin la necesidad de tener que exportar el código de nuestro editor para 
hacer pruebas en la simulación.

La versión utilizada es la v.1.4.2 y se puede encontrar en el siguiente enlace \footnote{\url{https://github.com/ajaxorg/ace-builds/tree/0d62c26de7b2e1922d8dd95ba587c9845c018c51}}.



\section{Librería Blockly}
\label{sec:blockly}

\emph{\emph{Blockly}} es una librería que permite aprender programación mediante el uso de bloques
visuales que se pueden combinar y traducir posteriormente a distintos lenguajes. Permite
a los usuarios programar en distintos lenguajes sin tener que conocer la sintaxis del lenguaje al 
detalle. Actualmente se encuentra en la versión v.1.20180629.0 y se puede encontrar en el siguiente enlace \footnote{\url{https://developers.google.com/blockly/guides/get-started/web}} disponible en distintos métodos de descarga.\\

Es una forma de aprender a programar orientada a estudiantes de temprana edad, el objetivo es que el 
alumno pueda aprender la lógica de los algoritmos de programación pero sin tener que aprender todo el
lenguaje como puede ser declaración de variables, tipo y en general la sintaxis propia del lenguaje
que puede ser pesado al principio.

\emph{Blockly} es la base de otros entornos de programación visual como \emph{\emph{Scratch 3}} ya
que es código libre lo que permite a cualquier desarrollador contribuir y utilizarlo en su propia 
aplicación. Ofrece métodos para importar y exportar el código de bloques además de métodos para 
modificar y personalizar el aspecto de los bloques para que sigan el estilo de la interfaz de 
la aplicación.

Es una tecnología orientada completamente al lado cliente y se puede utilizar mediante el fichero 
comprimido del repositorio oficial de \emph{Blockly} llamado \emph{blockly-compressed}, no 
utiliza dependencias y como ya hemos comentado es código libre.

\subsection{Generadores de código}
\label{subsec:blockly-generadores}

\emph{Blockly} provee de generadores de código para los siguientes lenguajes:JavaScript, Python, PHP, Lua, Dart.


Estos generadores proveen las herramientas básicas para crear funciones, expresiones lógicas, bucles,
etc. La problemática de esto es que a veces nuestras aplicaciones necesitan usar la API de otras 
dependencias como en nuestro caso. Para solventar esto \emph{Blockly} permite generar bloques 
personalizados que se traducirán a la instrucción necesaria dotando de mucha flexibilidad al entorno.\\

Además, \emph{Blockly} permite añadir palabras reservadas dentro de cada tipo de lenguaje lo cual 
permite un control de colisiones con las variables propias de la aplicación ya que en lugar de 
eliminar esta variable lo que hace \emph{Blockly} es renombrar todas las apariciones de la variable 
en el código generado dinámicamente.\\

Se puede apreciar que el código generado a través de los bloques será correcto en su sintaxis pero hay
una problemática presente en programación y es la aparición de \emph{bucles infinitos}, la librería 
dota de un método para intentar aliviar esta problemática, en la figura ~\ref{fig:loop-trap} se muestra
un simple código que cuenta el número de iteraciones del bucle, no es el mejor método para solventar el 
problema ya que, como es nuestro caso, necesitaremos bucles de ejecución continua pero dota a la 
aplicación de cierto control de ejecución de código.

\begin{figure}[h]
	\centering
	\includegraphics[width=14cm, height=1.2cm]{img/loop-trap.PNG}
	\caption{Implementación de un contador para evitar bucles infinitos al traducir lenguaje de 
	\emph{Blockly}}
	\label{fig:loop-trap}
\end{figure}

\subsection{Bloques personalizados}
\label{subsec:custom-blocks}

Además de generar código en distintos lenguajes\emph{Blockly} permite también crear bloques personalizados lo que posibilita generar código a través de 
bloques y conectarlos con cualquier tipo de API.\\

Para generar bloques personalizados \emph{Blockly} hemos de configurar varios aspectos:

\begin{itemize}
	\item Configuración de los parámetros de entrada y salida del bloque, conectores o parámetros
	en línea y color. La configuración del bloque se permite mediante dos formas distintas a través
	de un JSON o mediante JavaScript registrando un nuevo bloque en el objeto \emph{Blockly}. La 
	figura ~\ref{fig:block-definition} muestra ambos métodos para generar el mismo bloque.
	\begin{figure}[h]
		\centering
		\includegraphics[width=12cm, height=6cm]{img/block-json.PNG}
		\includegraphics[width=12cm, height=4cm]{img/block-js.PNG}
		\caption{Modos de configuración de un bloque personalizado en \emph{Blockly}, la declaración de las distintas partes del bloque es bastante intuitiva y los
		parámetros son autodescriptivos.}
		\label{fig:block-definition}
	\end{figure}
	
	\item Configuración de la traducción del bloque a la instrucción de interés en los distintos 
	lenguajes necesarios. 
	
	\item Iniciar el bloque para que se renderice en el editor de bloques visual. La figura 
	~\ref{fig:block-init}
	\begin{figure}[h]
		\centering
		\includegraphics[width=6cm, height=3cm]{img/block-init.PNG}
		\caption{Muestra del código que inicia el bloque para que se muestre en el editor visual, 
		\emph{moveBlock} representa un objeto JSON con la configuración del bloque}
		\label{fig:block-init}
	\end{figure}
	
\end{itemize}

Como se ve, aunque los parámetros del JSON son autodescriptivos y sencillos de entender es complicado
y lento generar un bloque desde cero por tanto \emph{Google} ofrece unas herramientas para 
desarrolladores en línea que permiten acelerar esta generación de código.


La figura ~\ref{fig:dev-tool} muestra una imagen del entorno de creación de bloques personalizados 
que provee \emph{Blockly} en el cual se puede observar en la parte de la derecha el archivo en formato
\emph{JSON} \emph{(JavaScript Object Notation)} de configuración del bloque en el que se definen las 
entradas que toma, el color con el que se mostrará entre otra serie de parámetros. Además, muestra
la función con la configuración básica para obtener los parámetros que se utilizarán para generar
código real JavaScript.\\

\begin{figure}[h]
	\centering
	\includegraphics[width=16cm, height=7cm]{img/custom-block.PNG}
	\caption{Herramienta de desarrollo para generación de bloques personalizados.}
	\label{fig:dev-tool}
\end{figure}

Como se ve en la figura ~\ref{fig:dev-tool} en la parte superior derecha la herramienta muestra cómo
se verá el bloque final en la aplicación bajo la configuración visual por defecto que trae 
\emph{Blockly}.

\subsection{Menú de bloques, \emph{Toolbox}}

El editor de \emph{Blockly} provee además de una barra de herramientas en la cual se muestran los 
bloques que podrán ser usados a través del editor. Estos bloques se configuran a través de un fichero 
XML en el cual podemos tener distintos tipos de etiquetas que se muestran a continuación.

\begin{figure}
	\centering
	\includegraphics[width=11cm, height=2cm]{img/toolbox-xml.PNG}
	\caption{Ejemplo de las etiquetas posibles de configuración de la \emph{toolbox} del editor}
	\label{fig:xml-toolbox}
\end{figure}

Como vemos en la figura ~\ref{fig:xml-toolbox} tenemos tres bloques distintos, la raíz del XML marcada
por la etiqueta \emph{xml}, la etiqueta \emph{category} que permite hacer divisiones de bloques por
tipos distintos, en la imagen tenemos dos categorías \emph{variables y texto}. La categoría 
\emph{variables} no tiene bloques dentro ya que se generan dinámicamente. En la categoría
\emph{texto} vemos declarado un bloque de tipo \emph{text} que representaría un \emph{String} en 
lenguaje JavaScript. Como se puede apreciar la configuración del menú de bloques no es fija, se puede
crear distintas categorías en función del diseño de la interfaz de usuario.




\section{Librería jQuery}
\label{sec:jquery}

\emph{\emph{jQuery}} es una librería multiplataforma de \emph{JavaScript}, creada inicialmente por 
John Resig y permite simplificar la forma de interactuar con los documentos HTML, el DOM, manejo de 
eventos, desarrollo de animaciones y agregar interacción ligera entre el cliente y servidor con el 
mecanismo \emph{AJAX (Asyncronous JSON And XML}. Es una biblioteca ampliamente utilizada debido
a las características que ofrece y sigue la filosofía \emph{' Code less, do more '}. Se encuentra en
la versión v3.3.1 y se puede encontrar en el siguiente enlace \footnote{\url{https://jquery.com/download/}}.\\


\emph{jQuery} es software libre de código abierto, posee una licencia doble MIT y una licencia pública
general de GNU v.2 lo que permite su uso en proyectos libres y privados. jQuery se basa en simplificar 
funcionalidad que se repite a menudo en las páginas web como por ejemplo manejo de evento 'click' de un
botón, ocultar o mostrar un elemento del DOM, etc.

Basados en \emph{jQuery} existen una gran cantidad de \emph{plugins} (extensiones) gratuitos y de pago
que permiten disminuir el tiempo de desarrollo de la interfaz de usuario como por ejemplo hacer
que una pagina web sea \emph{responsive} (que el contenido se adapte bien al tipo de dispositivo y 
navegador), crear una galería de fotos, carrusel de imágenes, etc. 

Una de las características de \emph{jQuery} más importante es su facilidad de uso. La curva de 
aprendizaje de jQuery es sencilla ya que como hemos comentado ofrecen métodos para manejo incluso de 
CSS. La figura ~\ref{fig:jquery} muestra cómo se hace un efecto de \emph{'fundido'} con jQuery, como 
se ve en la figura el método puede tomar parámetros de entrada para regular la velocidad a la que se
quiere realizar el efecto.

\begin{figure}[h]
	\centering
	\includegraphics[width=8.5cm, height=0.5cm]{img/jquery-fadein.PNG}
	\caption{Instrucción para hacer un efecto de \emph{'fundido'} en jQuery.}
	\label{fig:jquery}
\end{figure}
 
En la parte de la derecha de la instrucción tenemos el método a ejecutar y la velocidad, en la parte
de la izquierda tenemos el acceso completo al DOM para un elemento en concreto.\\

Para hacer este tipo de efectos es necesario conocer bien el uso del lenguaje CSS, jQuery permite 
a desarrolladores novatos en el campo de maquetación de interfaces de usuario hacer que sus páginas 
web sean algo más elaboradas en cuanto a efectos e interfaz se refiere sin la necesidad de conocimiento
profundo de CSS3.\\

Actualmente en el punto en el que se encuentra el desarrollo web \emph{jQuery} ha disminuido su uso
debido a la aparición de entornos como \emph{React, VueJS y AngularJS} que implementan
funcionalidad similar a la de jQuery pero además permiten implementación de patrones de diseños como
el \emph{MVC (Modelo Vista-Controlador)}. Pero este tipo de modelos no son necesarios en el proyecto 
que nos concierne de ahí la elección de jQuery en vez de estos otros entornos.


\section{NPM y Webpack}
\label{sec:npm}

\emph{\emph{NPM}} es la abreviatura de (Node Package Management), es una tecnología de gestión de 
dependencias del entorno NodeJS que permite la simplificación de instalación de las dependencias
de un determinado software.\\

La declaración de dependencias de la aplicación se hace en el fichero \emph{package.json} en el cual
se ponen los distintos paquetes NPM de los que hará uso nuestra aplicación tanto para un entorno de 
desarrollo como para un entorno de producción.

La instalación con NPM es simple, basta con moverse al directorio en el que se encuentra nuestro 
fichero \emph{package.json} y ejecutar \emph{npm install}, esto descargará todas las dependencias
de nuestra aplicación e incluso las dependencias de nuestras dependencias (si existieran) y las 
instalará bajo la carpeta \emph{node-modules}. La referencia en el nombre de \emph{Node} es debido
a que el entorno \emph{Node} tiene una simplificación para hacer uso de las librerías en la carpeta
\emph{node-modules} mediante la instrucción \emph{require} la cual busca dentro de la carpeta la que
se llame igual que la que pasamos como parámetro a la función.\\

Para la utilización de estas librerías en el lado cliente tenemos que hacer uso de ES6 y la 
instrucción \emph{import}. Esto unido con \emph{Webpack} permite que todas las dependencias en el lado
cliente (imports) se junten en un único fichero.

\emph{WebPack} es una herramienta de empaquetado de aplicaciones lo que permite generar un
\emph{bundle} con todo lo necesario de nuestra aplicación haciendo que el uso de nuestra aplicación
en el HTML se simplifique necesitando únicamente una etiqueta \emph{script} que referencie a nuestro
\emph{bundle}.\\


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% DISEÑO E IMPLEMENTACIÓN %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\cleardoublepage
\chapter{Simulador robótico web: \emph{WebSim}}
\label{chap:disenno}

En este capítulo se muestra la estructura diseñada y desarrollada para el simulador robótico web y se detallan sus partes
explicando la funcionalidad de cada una de ellas. El simulador se ha llamado \emph{WebSim}.

\section{Diseño}
\label{sec:arquitectura}

\begin{figure}[h]
  \centering
  \includegraphics[width=17cm, height=10cm]{img/Arquitectura-websim.PNG}
  \caption{Estructura de WebSim.}
  \label{fig:arquitectura}
\end{figure}

Como se comentó en el capítulo ~\ref{chap:intro}, el objetivo del presente proyecto es crear 
una aplicación para simular robots en el entorno \emph{A-FRAME} permitiendo conectar un 
editor de código para ejecutar instrucciones con el robot simulado. El primer componente de esta aplicación
es el propio simulador robótico. En la figura ~\ref{fig:arquitectura} se muestra el diseño que sigue el simulador WebSim. A continuación
se explicarán cada una de las partes por separado y la comunicación existente entre ellas.

La arquitectura sigue un diseño por módulos,
la intención es poder conectar distintos tipos web de entradas a WebSim como puede ser un editor web externo de 
código en JavaScript, un editor de código en el lenguaje visual Blockly, una aplicación externa al navegador web vía 
comunicaciones ICE, etc. sin tener una aplicación monolítica en la que todas las partes estén completamente acopladas.\\

WebSim tiene cinco funcionalidades principales:

\begin{itemize}
	\item Registra los tres componentes constituyentes del robot en \emph{A-FRAME}. Para la simulación del robot en el 
	entorno ha sido necesario crear tres componentes 
	llamados \emph{followBody, spectatorComponent, intersectionHandler} que se encargan  respectivamente de simular una cámara 
	contenida en el robot, manejar los eventos de intersección de láseres en el robot simulado y 
	anclar al cuerpo del robot simulado un elemento.
	
	\item Ofrece la interfaz de programación en JavaScript para manejar el robot simulado , llamada \emph{HAL API} 
	\emph{(Hardware Abstraction Layer)} lo que simplifica el uso del robot creado en A-FRAME. Se le 
	pueden enviar instrucciones que el robot ejecutará mediante una 
	única línea de código, lo que permitirá al usuario crear una lógica para el robot de manera limpia, sin
	tener que comunicarse directamente con el motor \emph{A-FRAME} ya que de eso se encarga \emph{WebSim}.
	
	\item Ofrece la instancia que contiene el objeto robot \emph{(myRobot)}. De manera que el usuario
	no tiene que instanciar ningún tipo de variable de la clase \emph{RobotI} ya que se le ofrece para
	el uso directo. Esto conlleva una doble funcionalidad. Primero, simplifica el uso del simulador aún
	más, ya que el usuario solo tiene que enviar instrucciones haciendo uso del objeto \emph{myRobot}.
	Y segundo, evita la creación de múltiples instancias del mismo objeto que llevaría a una incorrecta 
	ejecución.
	
	\item Controla la ejecución del \emph{mundo}, es decir, permite arrancar o pausar la simulación del robot
	en sí misma. 
	Se muestra en la figura ~\ref{fig:arquitectura} 
	mediante el bloque \emph{websim world controller}. Una de las funcionalidades del control del entorno
	es evitar que el usuario cambie el valor de la variable \emph{myRobot} lo cual haría que se perdiese
	el objeto robot y tendría que refrescar la página.
	
	\item Permite la conexión del robot simulado con software externo mediante intercambio de mensajes. De
	este modo el software 
	externo al navegador web puede acceder a los sensores y actuadores del robot simulado y gobernar su 
	comportamiento. El software externo por tanto se encargará únicamente de pedir datos de los sensores 
	simulados y enviar comandos a los actuadores simulados. Es un segundo interfaz de programación del robot simulado,
	no ya como funciones JavaScript sino como mensajes.


\end{itemize}

Con este diseño el simulador \emph{WebSim} provee de una capa de simplificación
y abstracción para los usuarios de modo que ellos solo se tienen que concentrar en generar un código
para ejecutar una serie de instrucciones en el robot accediendo de forma simple a los distintos sensores
como son la cámara, sensores de ultrasonido, sensores infrarrojos y a los motores. \\

Principalmente el usuario se encargará de programar la lógica del robot, es decir, el pensamiento 
(código) para resolver un ejercicio determinado con el objetivo de aprender programación de robots.



\section{Simulación con A-FRAME}
\label{sec:simulacion}

En esta sección se explica cómo se ha creado la simulación 3D de nuestra escena con el entorno 
\emph{A-FRAME}. \\

Como hemos explicado en la sección ~\ref{sec:A-FRAME}, \emph{A-FRAME} simplifica en gran parte la 
generación de escenas en el navegador, se monta sobre \emph{WebGL y three.js} y permite crear la escena
y sus integrantes (entidades) haciendo uso únicamente de etiquetas HTML. En la Figura 
~\ref{fig:escena-simulador} se muestra el esquema HTML necesario para generar nuestra escena simulada junto 
con el robot. El mundo simulado, es un conjunto de elementos 
con atributos y el motor \emph{A-FRAME} se encarga de su funcionamiento tridimensional.\\

\begin{figure}[H]
	\centering
	\includegraphics[width=16cm, height=9cm]{img/aframe-sim.PNG}
	\includegraphics[width=16cm, height=9cm]{img/aframe-sim-2.PNG}
	\caption{HTML que genera la escena principal del simulador}
	\label{fig:escena-simulador}
\end{figure}

\begin{figure}[h]
	\centering
	\includegraphics[width=5.5cm, height=2cm]{img/a-scene.PNG}
	\caption{Etiqueta \emph{a-scene}, es la etiqueta principal del entorno \emph{A-FRAME}.}
	\label{fig:a-scene}
\end{figure}

Como se ha comentado en el capítulo ~\ref{chap:herramientas}, la etiqueta \emph{a-scene} 
(Figura ~\ref{fig:a-scene}) es la etiqueta principal utilizada en \emph{A-FRAME} para generar la visualización. 
Como queremos una escena simple le hemos dado un color de fondo gris y hemos activado las físicas y las
mediciones. Las físicas permiten hacer uso del motor de físicas en el paquete \emph{A-FRAME-physics}
en el cual existe la gravedad, rozamiento y otros tipos de fuerzas haciendo que la simulación sea más realista. La etiqueta \emph{stats} (mediciones)
permite conocer las métricas relevantes de la escena como los \emph{FPS} (Fotogramas por segundo), el 
número de vértices que está manejando la escena, las texturas cargadas en la escena y los \emph{RAF
(Request Animation Frame)} que es la latencia de la escena. Estas medidas aparecen en rojo cuando 
el valor no es el adecuado para cada una de ellas, permiten al desarrollador medir la eficiencia de 
su escena y del código bajo ésta.\\

\begin{figure}[h]
	\centering
	\includegraphics[width=10cm, height=4.5cm]{img/a-assets.PNG}
	\caption{\emph{a-assets} es la etiqueta usada por \emph{A-FRAME} para la gestión de texturas y 
	recursos externos.}
	\label{fig:a-assets}
\end{figure}

Las etiquetas \emph{a-assets} (Figura ~\ref{fig:a-assets}) son un modo de \emph{A-FRAME} de gestionar de manera
más eficiente las texturas y modelos 3D a usar en las entidades de la escena. Podemos usar la etiqueta
\emph{a-asset-item} con cualquier tipo de archivo de entrada, llama al cargador de ficheros de 
\emph{three.js} y 'avisa' del estado de la carga del archivo, tiene tres estados:

\begin{itemize}
	\item \emph{Error}: Fallo al cargarlo.
	\item \emph{Progress}: lo emite cuando está en proceso de carga del archivo, devuelve un evento en el cual
	en el campo \emph{detail} tenemos un objeto de tipo \emph{XMLHTTPRequest} con la cantidad
	de bytes cargados en total.
	\item \emph{Loaded}: Indica que el archivo se ha cargado completa y correctamente.
\end{itemize}

En el mundo simulado de \emph{WebSim} se tiene: el cuerpo del robot, sus sensores de distancia, su cámara y
la escena que lo rodea. Todos estos objetos se detallan en las secciones siguientes.

\subsection{Cuerpo del robot}
\label{subsec:cuerpo}

El 'objeto' básico en una escena en 
\emph{A-FRAME} es \emph{a-entity} que es un objeto vacío al que le podemos conectar y dar
la funcionalidad que requiramos. Haciendo uso de esta etiqueta se ha simulado el robot \emph{Pibot}, que
se compone de ruedas con motores, sensores y cámara. La Figura ~\ref{fig:tags-pibot} muestra las 
etiquetas y atributos utilizados para la configuración inicial en la simulación del robot.\\

\begin{figure}[h]
	\centering
	\includegraphics[width=8cm, height=8cm]{img/pibot-tags.PNG}
	\caption{Etiquetas utilizadas para la simulación del robot en el entorno \emph{A-FRAME}.}
	\label{fig:tags-pibot}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=16cm, height=8cm]{img/pibot-simulado.PNG}
	\caption{PiBot simulado en la escena, se resalta la cámara integrada en él.}
	\label{fig:pibot-simulado}
\end{figure}

Para la simulación del cuerpo del robot hemos utilizado el atributo \emph{collada-model} que 
permite hacer uso de un modelo 3D en formato \emph{.dae} y anclarlo a nuestra entidad de la escena. Se
pueden usar otro tipo de formatos pero se ha elegido éste ya que es el que mejor rendimiento daba
al cargarlo en la escena. Además, se ha utilizado el atributo \emph{dynamic-body} 
que dota de físicas al elemento con la particularidad que entonces el elemento puede ser movido
por otros. 



\subsection{Sensores de distancia}
\label{subsec:sensores}

El robot tiene sensores de obstáculos, para los que se ha 
utilizado el atributo \emph{raycaster} que se encapsula dentro de la entidad con el identificador
\emph{positionSensor} (Figura ~\ref{fig:pos-sensor}) junto con el componente \emph{followBody} que se muestra en 
la figura ~\ref{fig:raycasters}. Estos sensores de distancia a obstáculos representan a sensores reales de ultrasonidos o de láser.
La carga y registro (Figura ~\ref{fig:custom-components}) de estos elementos se hace desde WebSim. Es necesario hacerlo así para poder modificarlos 
dinámicamente y añadir más o menos sensores de distancia al robot en tiempo de ejecución. Se ha utilizado un array de sensores de 
distancia que apuntan a distintas direcciones desde el cuerpo del robot.

\begin{figure}[h]
		\centering
		\includegraphics[width=13cm, height=1.6cm]{img/aframe-components.PNG}
		\caption{Registro de los tres componentes necesarios para el Robot simulado.}
		\label{fig:custom-components}
	\end{figure}

\begin{figure}[h]
	\centering
	\includegraphics[width=8cm, height=0.5cm]{img/pos-sensor.PNG}
	\caption{Etiqueta vacía utilizada para encapsular los sensores de distancia a obstáculos.}
	\label{fig:pos-sensor}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=14cm, height=7cm]{img/raycasters-renderizados.PNG}
	\caption{Etiquetas generadas por \emph{WebSim} para la simulación del sensor de ultrasonido.}
	\label{fig:raycasters}
\end{figure}

\subsection{Cámara}
\label{subsec:cam}

La cámara, como se ve en la Figura ~\ref{fig:pibot-simulado} es un elemento hijo 
encapsulado dentro de una entidad. Esto se hace así debido a las recomendaciones de \emph{A-FRAME} que indican 
que para modificar la posición de la cámara en tiempo de ejecución se incluya ésta dentro de una entidad. La etiqueta cámara cuenta 
con atributos entre los que destacan dos: \emph{spectator y wasd-controls-enabled}.

\begin{itemize}
	\item \emph{Spectator}: Este atributo ha sido creado específicamente para la aplicación y permite 
	imprimir el contenido de la cámara en una etiqueta \emph{canvas} dentro de la página
	HTML lo que permite mostrar lo que está viendo el robot en tiempo real. La Figura ~\ref{fig:espectador} muestra el campo de visión de la cámara integrada en el robot simulado y la imagen que capta.
	\begin{figure}[h]
  		\centering
  		\begin{minipage}[b]{0.4\textwidth}
    		\includegraphics[width=7cm, height=5cm]{img/robot-fov.png}
  		\end{minipage}
  		\hfill
  		\begin{minipage}[b]{0.4\textwidth}
    		\includegraphics[width=7cm, height=5cm]{img/Espectador.png}
  		\end{minipage}
  		\caption{Imagen captada por la cámara del robot simulado que hace uso del componente \emph{spectator}.}
  		\label{fig:espectador}
	\end{figure}
	
	\item \emph{WASD-Controls-Enabled}: Permite declarar si la cámara se podrá mover con las teclas W-A-S-D, 
	en nuestro caso no interesa ya que modificaríamos la posición de la cámara respecto del robot
	estropeando así la escena.
\end{itemize}

\subsection{Escena}
\label{subsec:escena}

En esta subsección se explica qué etiquetas se han utilizado para la generación de los elementos
que intervienen en la escena pero que no forman parte del cuerpo del robot. En la Figura 
~\ref{fig:etiquetas-escena} se muestran las etiquetas y atributos utilizados para el resto de elementos
de la escena.\\

\begin{figure}[h]
	\centering
	\includegraphics[width=10cm, height=12cm]{img/etiquetas-escena.PNG}
	\caption{Etiquetas utilizadas para crear diferentes elementos de la escena del simulador \emph{WebSim}.}
	\label{fig:etiquetas-escena}
\end{figure}

Por ejemplo, en uno de los ejercicios desarrollados la etiqueta \emph{a-sphere} declara la existencia de una esfera en nuestra escena, los atributos 
declaran las siguientes propiedades de la esfera, es de la clase \emph{'collidable'}. Esto se utiliza para
indicar al robot que los sensores de ultrasonido van a detectar intersecciones con este tipo de objetos.
La esfera también tiene físicas aplicadas y tiene una masa de 1000kg.

La etiqueta \emph{a-plane} indica la existencia de un plano horizontal. Este plano tiene físicas pero
es un cuerpo estático, es decir, no puede ser movido por otros objetos de la escena. Además, se
declara el tamaño del plano, la textura a cargar en el plano que en el caso de uno de los ejercicios será un circuito en 
blanco y negro y el número de veces que se repite a lo largo de los ejes X y Z. Como se indica que
solo se repita 1 vez en ambos ejes entonces \emph{A-FRAME} estira la textura hasta ajustarla con al tamaño del 
plano.

Por último tenemos dos etiquetas, \emph{a-light} que indica que se va a utilizar una luz en la escena
de tipo ambiental de color blanco y la etiqueta \emph{a-entity} que declara una entidad vacía 
que, como se ha explicado antes, se usa para posicionar la cámara observadora principal desde la que se ve
la escena. La Figura ~\ref{fig:esfera} muestra los elementos mencionados renderizados en la escena, la luz ambiental no puede mostrarse físicamente.

\begin{figure}
\centering
\includegraphics[width=16cm, height=9cm]{img/esfera.png}
\caption{Representación en la escena de los elementos que componen la escena.}
\label{fig:esfera}
\end{figure}


\section{Drivers del robot}
\label{sec:drivers}

El robot consta de varios sensores y actuadores, simulados por \emph{A-FRAME}. Los drivers permiten a código
externo al simulador acceder a los sensores y actuadores simulados a través de un
interfaz de programación llamado \emph{HAL-API (Hardware Abstraction Layer)}. 
Están escritos en JavaScript y también materializan la simulación del robot (por ejemplo su movimiento) 
manejando la representación web del robot en \emph{A-FRAME}.\\


Por ejemplo, los sensores implementados en el robot recogen una serie de datos que han de ser guardados 
de manera interna en el robot para que el usuario pueda acceder a ellos y programar una lógica.
Para ello se crean una serie de métodos en JavaScript que se conectan con el robot simulado en
A-FRAME y se crean variables internas para guardar los datos y servirlos mediante una instrucción simple.\\

En el robot dentro de \emph{WebSim} tenemos dos hebras independientes, la hebra de los motores y la hebra de obtención de datos de la cámara. Son funciones autónomas periódicas y extraen o añaden datos
en ciertas variables internas del robot, lo que permite hacer uso de estos datos de manera asíncrona.


\subsection{Constructor}

Esos métodos JavaScript y variables internas mencionados se materializan como un objeto de la clase \emph{RobotI}.
El constructor de la clase \emph{RobotI} es un método obligatorio en \emph{JavaScript}, es el primer
método al que se llama una vez se crea una instancia del objeto. El constructor de nuestro objeto toma como 
parámetro de entrada un \emph{string} con el identificador de la etiqueta HTML (etiqueta del entorno A-FRAME) 
en la cual tenemos el robot simulado. Gracias a esto se puede hacer uso desde \emph{JavaScript} de las 
propiedades que ofrece el entorno A-FRAME, como la posición en la escena, añadir o eliminar elementos 
en la escena, obtener la imagen de la cámara, etc. \\

Además de acceder al robot simulado de A-FRAME, el constructor inicia una serie de 
variables internas de configuración del robot (\emph{this} hace referencia al contexto del objeto robot):

\begin{itemize}
	\item \emph{defaultDistanceDetection}: Es una variable de configuración de los sensores de 
	ultrasonido que declara la distancia máxima a la cuál los sensores detectarán un objeto.
	Se ha elegido hasta 10 metros.
	
	\item \emph{defaultNumOfRays}: Es una variable de configuración de los sensores de distancia a obstáculos.
	Declara el número de rayos que se simularán para detectar objetos. El arco que abarcamos es 180º por lo
	tanto cuanto mayor sea el número más precisa angularmente será la detección de objetos.
	
	\item \emph{this.robot}: Es una variable que permite crear un enlace entre la abstracción del 
	objeto robot y el robot simulado en A-FRAME y permite acceder a los distintos
	métodos que ofrece A-FRAME por defecto.
	
	\item \emph{this.activeRays}: Es una variable de control de tipo \emph{boolean} que permite
	saber si los láseres del sensor de ultrasonidos están activos o no. Esta variable permite 
	hacer un apagado o encendido de los láseres en caliente, cosa que no suele ser habitual en
	robótica pero ya que nos encontramos en un entorno de simulación puede ser interesante la 
	posibilidad de apagar estos láseres con el fin de reducir la cantidad de código a ejecutar por
	el robot y hacer así que el rendimiento mejore.
	
	\item \emph{this.distanceArray}: Se trata de un objeto \emph{JavaScript} que contiene tres
	variables de tipo \emph{Array} en las cuales se guardan las distancias que detectan los sensores
	de ultrasonidos agrupados en tres conjuntos; centro, izquierda y derecha lo que permite 
	conocer la ubicación del objeto que se está detectando.
	
	\item \emph{this.understandedColors}: Es una variable interna del robot que permite asociar un color de 
	entrada como tipo \emph{string}
	a sus valores de filtros de color RGB (Red-Green-Blue) para detectarlo mediante el uso de 
	herramientas de OpenCVjs. Esto permite simplificar el uso de la detección de objetos mediante el nombre de
	color para usuarios que no tengan conocimientos en visión artificial. Actualmente se implementan las
	componentes primarias y el color blanco.
	
	\item \emph{this.velocity}: Es una variable de configuración de la velocidad inicial del robot
	en los distintos ejes (por defecto cero en todos ellos). Esta variable es importante ya que 
	a la hora de programar la lógica del robot es necesario conocer la velocidad actual. Esto 
	permite tenerla guardada para su uso tanto por los motores como por los usuarios. Tenemos 3 velocidades 
	distintas en función de cada eje (X-Y-Z). En el eje X tendremos la velocidad en el plano horizontal, la 
	velocidad lineal. De manera simplificada sería la velocidad a la que se mueve el robot en la dirección en 
	la que mira. En el eje Y tendremos la velocidad en la cual se eleva el robot (debido al sistema
	de coordenadas tomado en A-FRAME donde Y es la altura). Esta velocidad no es utilizada en la implementación
	actual del robot. En
	el eje Z tendremos la velocidad de giro.
\end{itemize}


Por último, el constructor del robot llama a los métodos de arranque de motores, cámara y sensor de 
ultrasonido que se explicarán más detalladamente en sus correspondientes subsecciones a continuación. La Figura ~\ref{fig:constructor-robot} muestra el código del constructor del robot. La variable 
\emph{this.understandedColors} es una simplificación de los filtros a aplicar en 
una imagen para detectar un objeto con dicho color.
\begin{figure}[h]
	\centering
	\includegraphics[width=11.5cm, height=11.5cm]{img/robot-constructor.PNG}
	\caption{Código del constructor del objeto robot, la variable \emph{this} hace referencia al 
	contexto.}
	\label{fig:constructor-robot}
\end{figure}


\subsection{Driver de motores}

La función principal de los motores es permitir el movimiento del robot en el plano horizontal a
la velocidad configurada por el usuario, por ejemplo mediante el método setV. Además, un objetivo secundario para esta interfaz es que los
motores funcionen de manera autónoma, es decir, que no tengamos que enviar constantemente instrucciones
al robot para que este se mueva sino que al configurar la velocidad al robot este se mueva
de manera autónoma hasta nueva orden. Se tienen unas variables internas de velocidades comandadas que se consultan para
materializar el movimiento simulado y se actualizan desde los métodos del HAL API
a través de los cuales se ordenan comandos de movimiento.\\ 

Inicialmente se llama desde el constructor del objeto robot y, una vez arrancado, los pasos que ejecuta
son los siguientes:

\begin{itemize}
	\item Se obtiene la orientación actual, esto se hace debido a que, como el robot se encuentra en un 
	sistema de coordenadas, es necesario saber hacia donde mira el robot para poder moverlo hacia adelante y 
	atrás correctamente. 
	
	\item Cálculo de la nueva posición. Se calcula 
	la nueva posición en función de la velocidad lineal configurada en ese instante y en función del 
	vector de vista (hacia dónde mira el robot). Para calcular dicha posición es necesaria por tanto la 
	orientación,
	la velocidad lineal y la posición actual del robot. Se ha utilizado una descomposición de vectores
	en sus componentes en el eje X-Z para calcular la nueva posición
	\begin{figure}[h]
		\centering
		\includegraphics[width=11cm, height=2.5cm]{img/pos-equation.PNG}
		\caption{Cálculo de la nueva posición en función de la orientación y la velocidad lineal.}
	\end{figure}
	
	\item Se establece la nueva posición para el objeto en la escena. En cada iteración se establece la nueva posición
	previamente calculada en la escena y la velocidad angular del robot.
	
	\item Se establece un temporizador para que la función se llame así misma. Esto se hace para que
	la función se invoque así misma constantemente y no tener que estar enviando continuamente 
	instrucciones de velocidad, lo que simplifica el código de la aplicación de control del robot.
\end{itemize}


Gracias a este temporizador nativo de \emph{JavaScript} se puede crear una función iterativa. Con ello
el modo de usar los motores se simplifica: el usuario únicamente llama asíncronamente a la función \emph{setV}
que guarda en la variable interna \emph{this.velocity} la velocidad pasada como parámetro a la función
y será la función iterativa del motor la que se encargará de comprobar periódicamente este registro para saber
cuál es la velocidad especificada por el usuario. Esto permite simular una hebra dentro del navegador web y
conjuga el funcionamiento típico de eventos del entorno web con el funcionamiento iterativo típico de los
controladores robóticos.\\

Otros métodos relacionados con los motores del robot son:
\begin{itemize}
	\item \emph{getV}: Método para que el usuario conozca la velocidad lineal actual ordenada al robot.
	
	\item \emph{getW}: Método para que el usuario conozca la velocidad angular actual ordenada al robot.
	
	\item \emph{getL}: Método para que el usuario conozca la velocidad de elevación ordenada al robot.
	
	\item \emph{setV}: Método para que el usuario comande la velocidad lineal del robot.
	
	\item \emph{setW}: Método para que el usuario comande la velocidad angular del robot.
	
	\item \emph{setL}: Método para que el usuario comande la velocidad de elevación del robot.
	
	\item \emph{move}: Método que combina la funcionalidad de \emph{setV} y \emph{setW} simultáneamente.
	
\end{itemize}

En la Figura ~\ref{fig:motor-esquema} se presenta un esquema de la ejecución de la 
función \emph{setVelocity} del robot.

\begin{figure}[H]
	\centering
	\includegraphics[width=16cm, height=9.7cm]{img/EJECUCION-MOTOR.PNG}
	\caption{Esquema de ejecución de la función \emph{setVelocity}, en la parte superior izquierda
	de la imagen se muestran algunos datos que existen en el contexto de la función.}
	\label{fig:motor-esquema}
\end{figure}

Finalmente se muestra en la Tabla ~\ref{tab:tabla-motores} el resumen de las funciones que se ofrecen en el \emph{HAL API} para usar los motores en el robot simulado.

\begin{table}[h!]
  \begin{center}
    \caption{Métodos (HAL API) de los motores del robot.}
    \vspace{0.5cm}
    \label{tab:tabla-motores}
    \begin{tabular}{|c|c|} % <-- Alignments: 1st column left, 2nd middle and 3rd right, with vertical lines in between
    \hline
      \textbf{Método} & \textbf{Descripción}\\
      \hline
.setV(velLineal) & \begin{tabular}[c]{@{}c@{}}Mueve hacia delante o atrás el robot.\\ INPUT:\\ - velLineal: número con la velocidad lineal(m/s).\end{tabular} \\ \hline
.setW(velAngular) & \begin{tabular}[c]{@{}c@{}}Hace girar al robot.\\ INPUT:\\ - velAngular: número con la velocidad angular (rad/s).\end{tabular} \\ \hline
.setL(velElevacion) & \begin{tabular}[c]{@{}c@{}}Hace que el robot se mueva hacia arriba.\\ INPUT:\\ - velElevacion: numero con la velocidad de elevación (m/s)\end{tabular} \\ \hline
.move(velLineal, velAngular) & \begin{tabular}[c]{@{}c@{}}Mueve el robot hacia delante/atrás y gira al mismo tiempo.\\ INPUT:\\ Los mismos parámetros que en setV y setW.\end{tabular} \\ \hline
.getV() & \begin{tabular}[c]{@{}c@{}}Obtener la velocidad lineal configurada en el robot.\\ OUTPUT:\\ number\end{tabular} \\ \hline
.getW() & \begin{tabular}[c]{@{}c@{}}Obtener la velocidad angular configurada en el robot.\\ OUTPUT:\\ number\end{tabular} \\ \hline
.getL() & \begin{tabular}[c]{@{}c@{}}Obtener la velocidad de elevación configurada en el robot.\\ OUTPUT:\\ number\end{tabular} \\ \hline
    \end{tabular}
  \end{center}
\end{table}

\subsection{Driver del sensor de distancia a obstáculos}

El sensor de ultrasonido o láser permite detectar obstáculos abarcando un 
radio de 180º por delante del frontal del robot. Este sensor 
ha sido simulado mediante el atributo \emph{raycaster} existente en A-FRAME que se asemeja a un láser y
nos permite conocer el punto de intersección entre un rayo 3D y un determinado objeto.

Para la simulación de este sensor tenemos dos partes:
\begin{itemize}
	\item Componente A-FRAME \emph{followBody}: Este componente tiene una función muy simple, anclar 
	al robot un componente de tipo \emph{raycaster}. En este caso sin tener que añadirle físicas, ya que
	al aplicarle físicas a una entidad se le aplican también a todos sus elementos 'hijo' dentro del
	HTML. Este componente permite que los \emph{raycasters} sigan la posición del robot manteniendo
	su orientación relativa respecto del robot.
	
	\item Componente A-FRAME \emph{intersectionHandler}: Este componente es el más importante ya que
	permite manejar correctamente el evento que se dispara cuando hay una intersección para un 
	\emph{raycaster}. Los eventos tienen todos el mismo nombre y además el número de identificación del 
	\emph{raycaster} que lanza el evento. Esto permite detectar qué \emph{raycaster}
	lanza el evento y guardar correctamente la distancia que detecta.
	
	\item \emph{startRaycasters}: Función interna del objeto robot que sirve para iniciar los
	\emph{raycasters} dándoles su ángulo de orientación respecto al robot y agrupándolos en función 
	de su posición (izquierda - centro - derecha). Les da un identificador numérico unívoco que permite
	crear un evento individual para cada uno de ellos y además, una vez creados los \emph{raycasters}, 
	llama a la función que declara los escuchadores de eventos individuales para cada uno de ellos. Para 
	conocer si los \emph{raycasters} están activos o no se hace uso de una variable de estado 
	\emph{this.activeRays} que indica si los \emph{raycasters} están en funcionamiento o no.
	
	\item \emph{createRaycasters}: Función que crea un \emph{raycaster} configurando una serie de
	atributos como la distancia máxima a la que se detectarán intersecciones. Además se le añaden los
	componentes anteriormente mencionados \emph{followBody} e \emph{intersectionHandler}.
	
	\item \emph{stopRaycasters}: Función que elimina todos los elementos \emph{raycaster} del robot,
	es decir, detiene el sensor de obstáculos.
	
	\item \emph{setListener}: Declara dos escuchadores de eventos, uno para el evento 
	\emph{intersection-detected-'id del raycaster'} y otro para el evento 
	\emph{intersection-cleared-'id del raycaster'} junto con las funciones a las que llamar cuando
	ocurra cada uno de los dos eventos. Cuando el evento \emph{intersection-detected-'id del raycaster'} 
	ocurre se llama a la función \emph{updateDistance} y cuando ocurre el evento \emph{intersection-cleared-'id del raycaster'} se llama a la función \emph{eraseDistance}.
	
	\item \emph{removeListener}: Función que elimina el escuchador de eventos, es llamada desde la
	función \emph{stopRaycasters}.
	
	\item \emph{updateDistance}: Función que se llama cuando se detecta el evento 
	\emph{intersection-detected-'id del raycaster'}. Esta función actualiza el array de distancias
	\emph{(this.distanceArray)} detectadas por los \emph{raycasters}. En cada posición del array de distancias
	se guarda un objeto JavaScript con dos campos: \emph{id} es un identificador numérico vinculado al 
	\emph{raycaster} que ha emitido el evento anteriormente mencionado, \emph{d} es la distancia a la que se
	encuentra el objeto detectado por el \emph{raycaster} que ha emitido el evento.  
	
	\item \emph{eraseDistance}: Función que se lanza cuando se detecta el evento
	\emph{intersection-cleared-'id del raycaster'}. Esta función elimina el registro del array de 
	distancias para el \emph{raycaster} que lanza el evento y elimina dicho registro.
	
	\item \emph{getDistance} y \emph{getDistances}: Estas dos funciones son las que se proporcionan al  
	usuario final. Ambas funciones del HAL API devuelven las distancias detectadas por los \emph{raycasters}. 
	La diferencia principal es que la primera función únicamente devuelve la distancia que detecta
	el \emph{raycaster} central mientras que la segunda devuelve las distancia para todos ellos.
\end{itemize}

El inicio de los sensores de obstáculo es simple, el constructor llama a la función 
\emph{startRaycasters} y ésta se encarga de configurar todo llamando a las funciones mencionadas
anteriormente.\\

La Tabla ~\ref{tab:tabla-raycasters} muestra las funciones del HAL API que se ofrecen al usuario para hacer uso de 
los datos recogidos por los sensores de obstáculos.

\begin{table}[H]
  \begin{center}
    \caption{Métodos (API) de los sensores de obstáculos del robot.}
    \vspace{0.5cm}
    \label{tab:tabla-raycasters}
    \begin{tabular}{|c|c|} % <-- Alignments: 1st column left, 2nd middle and 3rd right, with vertical lines in between
    \hline
      \textbf{Método} & \textbf{Descripción}\\
      \hline
.getDistance() & \begin{tabular}[c]{@{}c@{}}Permite obtener la distancia del objeto que tiene delante\\ OUTPUT:\\ number (metros)\end{tabular} \\ \hline
.getDistances() & \begin{tabular}[c]{@{}c@{}}Permite obtener la distancia de los objetos detectados\\  en un arco de 180º, devuelve 31 valores por defecto\\ OUTPUT\\ Lista con 31 valores de tipo number (metros).\end{tabular} \\ \hline
		\end{tabular}
	\end{center}
\end{table}

\subsection{Sensores de infrarrojos}

El robot simulado está equipado con dos sensores ficticios de infrarrojos apuntando hacia el suelo. Se han
implementado mediante el uso de una cámara específica (Figura ~\ref{fig:pibot-simulado}) integrada en el cuerpo del robot por simplicidad y por mejora de rendimiento. Lo que se hace internamente es recortar la imagen de la 
cámara hasta quedarse con los píxeles que se encuentran más abajo en la imagen. El ancho de la imagen se 
mantiene, la imagen recortada tiene unas dimensiones de 5px de alto y 150px de ancho lo que permite que el 
robot detecte únicamente lo que tiene inmediatamente debajo. 

La función del HAL API \emph{readIR} es la encargada de simular este sensor de infrarrojo. Toma como parámetro de entrada un color como \emph{string} y accede a la variable anteriormente mencionada 
\emph{understandedColors} para obtener los valores de los filtros para el color seleccionados. Después recorta la imagen para obtener una imagen de 5x150 px. Posteriormente filtra por color la imagen para obtener únicamente 
la línea a seguir y calcula el centro de la línea mediante las funciones \emph{findContours y moments}
de la librería OpenCVjs.

La salida de la función son valores entre 0 y 3 que representan lo siguiente:
\begin{itemize}
	\item \emph{0}: Los dos sensores ficticios infrarrojos están detectando la línea. En la simulación de los sensores se emite este valor cuando el centroide de la línea se encuentra entre los píxeles 57 y 93. 
	
	\item \emph{1}: Únicamente el sensor infrarrojo de la izquierda detecta la línea. El valor cuando el centroide de la línea se encuentra entre los píxeles 0 y 57.
	
	\item \emph{2}: Únicamente el sensor infrarrojo de la derecha detecta la línea. El valor cuando el centroide de la línea se encuentra entre los píxeles 93 y 150.
	
	\item \emph{3}: Ninguno de los sensores detecta la línea. El valor se emite cuando no se encuentra el centroide de la línea en la imagen de la cámara del robot simulado.
\end{itemize}

Este formato de salida se debe a la implementación existente del HAL API en el robot real, para ser compatible con ella.
Un objetivo es que el mismo programa creado en el robot simulado se pueda exportar al robot real y funcione de la misma manera, por tanto convenía mantener compatibilidad para que las aplicaciones funcionen en ambos sin que sean necesarios grandes cambios. En la Tabla ~\ref{tab:tabla-ir} se explica la función que hace uso de
los sensores infrarrojos ficticios y explica su parámetro de entrada y salida.


\begin{table}[H]
  \begin{center}
    \caption{Métodos (HAL API) de los sensores infrarrojos del robot simulado.}
    \vspace{0.5cm}
    \label{tab:tabla-ir}
    \begin{tabular}{|c|c|} % <-- Alignments: 1st column left, 2nd middle and 3rd right, with vertical lines in between
    \hline
      \textbf{Método} & \textbf{Descripción}\\
      \hline
.readIR(color) & \begin{tabular}[c]{@{}c@{}}Obtiene valores entre 0-3 \\ en función de lo que detecte cada
uno de los sensores ficticios\\ INPUT:\\ -color: color a filtrar en la imagen como string\end{tabular} \\ \hline
		\end{tabular}
	\end{center}
\end{table}


\subsection{Driver de los sensores de odometría}

Los sensores de odometría se encargan de obtener la posición absoluta del vehículo durante la navegación. Para la simulación de estos sensores no ha sido necesario ningún componente extra ni entidad, se ha hecho uso del sistema de coordenadas y rotación de \emph{A-FRAME} que permite la obtención de estos datos a través de su API
JavaScript. De esta manera se han podido desarrollar dos funciones que simplifican el acceso a los datos, y se describen en la Tabla ~\ref{tab:tabla-odometria}.

\begin{table}[H]
  \begin{center}
    \caption{Métodos (HAL API) de los sensores de odometría del robot simulado.}
    \vspace{0.5cm}
    \label{tab:tabla-odometria}
    \begin{tabular}{|c|c|} % <-- Alignments: 1st column left, 2nd middle and 3rd right, with vertical lines in between
    \hline
      \textbf{Método} & \textbf{Descripción}\\
      \hline
.getRotation() & \begin{tabular}[c]{@{}c@{}}Devuelve un objeto con la orientación\\ del robot en los 3 ejes\\ OUTPUT:\\ \{\\ x: rotacionX (radianes)\\ y: rotacionY (radianes)\\ z: rotacionZ (radianes)\\ \}\end{tabular} \\ \hline
.getPosition() & \begin{tabular}[c]{@{}c@{}}Permite obtener la posición del robot en la escena\\ OUTPUT:\\ \{ \\ x: coordenadax (metros)\\ y: coordenaday (metros)\\ z: coordenadaz (metros)\\ theta: rotacion eje Y (horiz, radianes)\\ \}\end{tabular} \\ \hline
		\end{tabular}
	\end{center}
\end{table}


 La Figura ~\ref{fig:A-FRAME-axis} muestra los ejes utilizados del sistema de coordenadas del que hace uso \emph{A-FRAME} para entender 
los datos que se obtienen en estas funciones del HAL API.

\begin{figure}[h]
	\centering
	\includegraphics[width=7cm, height=7cm]{img/axis-aframe.png}
	\includegraphics[width=7cm, height=7cm]{img/robot-axis.png}
	\caption{Sistema de ejes utilizado en las escenas del entorno \emph{A-FRAME}}
	\label{fig:A-FRAME-axis}
\end{figure}

\subsection{Driver de la cámara}

Para la simulación de la cámara del robot se utiliza el componente \emph{spectator} que emplea
un \emph{renderer} de la librería \emph{three.js} para obtener la imagen de la escena. Posteriormente,
en el objeto robot se crean una serie de métodos que permiten configurar y acceder a los datos de la
cámara. A continuación se enumeran los métodos y se explica su función.

\begin{itemize}

	\item \emph{startCamera}: Esta función comprueba si la etiqueta con ID 'spectatorDiv' tiene una
	etiqueta 'hijo' que es una etiqueta de tipo \emph{canvas} donde se representa la cámara. Una vez
	haya cargado la etiqueta se accede al DOM para tomar el contenido de la etiqueta y se llama a la
	función \emph{getImageData-async}.
	
	\item \emph{getImageData-async}: Función que se ejecuta de manera similar a la función 
	\emph{setVelocity}, toma la imagen de manera autónoma haciendo uso de la función \emph{imread} de la
	librería OpenCVjs y guarda los datos de la imagen en la variable \emph{this.imagedata}. Por último
	se llama a si misma la función pasados 33 milisegundos (30 FPS). Sería una hebra más del simulador.
	
	\item \emph{getImage}: Esta función del HAL API la invocará el usuario final para obtener los datos de la cámara
	y con ello materializar la lógica que se necesite.
	
\end{itemize}

Además de estos métodos 'crudos' se ofrecen en el HAL API de \emph{WebSim} una serie de métodos algo más elaborados, 'cocinados', 
que se enumeran y explican a continuación:

\begin{itemize}
	\item \emph{getObjectColor y getObjectColorRGB}: Ambas funciones resuelven el mismo problema pero
	con parámetros de entrada distintos. Su función es filtrar un objeto que detecta la cámara
	del robot mediante su color. Devuelve un objeto con las coordenadas del centro del objeto en la 
	imagen y el área del objeto en la imagen. \emph{getObjectColor} toma como parámetro de entrada 
	el color del objeto a detectar como cadena de caracteres, \emph{getObjectColorRGB} toma como valores
	de entrada los filtros de color que se van a usar para detectar un elemento en la imagen.
	
\end{itemize}

En la Tabla ~\ref{tab:tabla-camara} se resumen las funciones que hacen uso de la cámara y se muestran sus
parámetros de entrada y salida.

\begin{table}[H]
  \begin{center}
    \caption{Métodos (API) para obtener datos de la cámara del robot simulado.}
    \vspace{0.5cm}
    \label{tab:tabla-camara}
    \begin{tabular}{|c|c|} % <-- Alignments: 1st column left, 2nd middle and 3rd right, with vertical lines in between
    \hline
      \textbf{Método} & \textbf{Descripción}\\
      \hline
.getImage() & \begin{tabular}[c]{@{}c@{}}Obtener la imagen de la cámara en el robot\\ OUTPUT:\\ cv.Mat() con la imagen de la cámara del robot.\end{tabular} \\ \hline
.getObjectColor(color) & \begin{tabular}[c]{@{}c@{}}Devuelve un objeto JavaScript con datos\\ sobre el objeto que detecte la cámara\\ con el color pasado como parámetro. \\ INPUT:\\color: color como string.\\ OUTPUT:\\ \{center: {[}cx, cy{]}, area: areaInt \}\end{tabular} \\ \hline
.getObjectColorRGB( filtroBajo, filtroAlto) & \begin{tabular}[c]{@{}c@{}}Devuelve un objeto JavaScript con datos\\ sobre el objeto que detecte la cámara\\ con el color pasado como parámetro. \\ INPUT:\\ filtroBajo: lista de longitud 4 con valores 0-255 (RGBA)\\ filtroAlto: lista de longitud 4 con valores de 0-255 (RGBA)\\ OUTPUT: \\ \{ center: {[}cx, cy{]}, area: areaInt \}
\end{tabular} \\ \hline
		\end{tabular}
	\end{center}
\end{table}

\section{Control de la simulación}
\label{sec:control}

Una de las funcionalidades de \emph{WebSim} es el control de ejecución de código 
en el robot simulado. Para esto se utiliza una variable dentro del simulador de tipo
\emph{boolean} que indica si el robot ya se está usando, si ya tiene un código ejecutando en ese momento. WebSim se encuentra a la espera de recibir comandos para el robot simulado mediante el evento\emph{code-to-run}, una vez ocurre el evento WebSim llama a la función interna \emph{startStopCode} en la cuál se comprueba si el robot ya se encuentra ejecutando un código. Si el robot está ejecutando lo que hace \emph{WebSim} es parar el robot, es decir, ponerle la velocidad a 0 
tanto lineal como angular y parar el hilo de ejecución del código que ha programado el usuario. De esta manera
implementamos la 'pausa académica' que permite detener el robot, modificar una parte del código de su aplicación y continuar la ejecución con el algoritmo de la aplicación modificada sin tener que refrescar la página del simulador ni rehacer el código desde el principio.\\

Si el robot no tiene ningún código ejecutando actualmente entonces \emph{WebSim} permite la ejecución del código mediante el 
uso de la instrucción nativa de JavaScript \emph{eval()}. Es conocido que esta instrucción tiene una 
vulnerabilidad que permite la inyección de código maligno. Se ha estudiado esta vulnerabilidad y en
el contexto que nos atañe no habría problema ya que es una aplicación totalmente local y todo código
malicioso que se quisiese inyectar se inyectaría únicamente en la máquina local.


\section{Conexiones de \emph{WebSim} con software externo}
\label{sec:conexion}

Como se ha señalado en la sección ~\ref{sec:arquitectura}, \emph{WebSim} permite también la conexión con 
software externo para manejar el robot simulado mediante mensajes recogiendo medidas sensoriales y enviando órdenes a los 
motores. Esta funcionalidad se implementa aprovechando una de las características principales
de JavaScript y es  su orientación a \emph{eventos}. Esto permite lanzar un evento (suceso) y escucharlo 
desde otra parte o método de la aplicación pudiendo ejecutar un código o incluso enviar datos en dicho evento.\\

En este caso se ha aprovechado esta funcionalidad para que \emph{WebSim} ofrezca un método de entrada en el
cual espera que se le pase un código en formato 'string' que posteriormente él se encargará de interpretar
y ejecutar comprobando previamente si el robot simulado
ya tiene un código en ejecución. Esto permite por tanto la conexión con editores embebidos en la página e incluso conectar \emph{WebSim} con una aplicación externa al navegador conectándose a través de
\emph{Websockets} como puede ser una aplicación ROS o aplicaciones con otro tipo de protocolos de comunicación como ICE.

El soporte para interfaces (ROS y ICE) se ha programado en WebSim pero no forman parte del núcleo de este
Trabajo Fin de Grado, por lo que apenas se describe un esbozo de este
bloque software del simulador robótico. La forma de hacer esta transmisión de mensajes hacia \emph{WebSim} es
un programa JavaScript que se suscribe a los canales tanto de ICE como de ROS y traduce los mensajes 
a un código que \emph{WebSim} interpretará y ejecutará en el robot simulado enganchando con el HAL API
existente del robot.

\section{Empaquetado e instalación}
\label{sec:empaquetado}

Una vez descrita la implementación de WebSim en esta sección se detallan dos aspectos importantes para facilitar su uso por terceros: el empaquetado y su instalación.

\subsection{Empaquetado con WebPack}

Para empaquetar \emph{WebSim} se ha hecho uso de los \emph{import} de ES6 y la herramienta \emph{WebPack} que permite 
generar un único fichero con todas las dependencias de un aplicación y, en modo producción, permite también 
minificar (reducir) el código de la aplicación una vez empaquetado para optimizar el rendimiento al 
cargar la página por parte de un usuario. 

Lo primero que ha sido necesario es instalar las dependencias a través de la herramienta NPM, lo que
hace que se pueda importar posteriormente desde el punto principal lo que será el empaquetado de dicha
dependencia. Desde el archivo \emph{websim.js} se importan explícitamente las dependencias de
\emph{A-FRAME, A-FRAME-PHYSICS y jQuery}, además de exportar desde otros archivos las clases y funciones
necesarias en el archivo principal.\\

La aplicación se empaqueta con WebPack y gracias a esta combinación se tiene un único \emph{bundle}
en el cual se incluirán todas las dependencias. A continuación se muestra en la figura 
~\ref{fig:webpack} el fichero de configuración de \emph{WebPack} para empaquetar WebSim.

\begin{figure}[h]
	\centering
	\includegraphics[width=7cm, height=12cm]{img/webpack.PNG}
	\caption{Fichero de configuración de la herramienta WebPack.}
	\label{fig:webpack}
\end{figure}

En la Figura ~\ref{fig:webpack} se muestra el archivo 'webpack.config.js', este archivo declara la configuración
de la herramienta \emph{WebPack}. A continuación se explica con más detalle cada una de las partes y el papel
que desempeña en el empaquetado final.\\

\begin{figure}[h]
	\centering
	\includegraphics[width=6.5cm, height=3cm]{img/entry-webpack.PNG}
	\caption{Ficheros de entrada que tomará \emph{WebPack} para hacer el empaquetado.}
	\label{fig:entry}
\end{figure}

La Figura ~\ref{fig:entry} muestra cuáles son los archivos en los cuales \emph{WebPack} comenzará el empaquetado
. En esos archivos existe una instrucción 'import' que declara una dependencia del código, por tanto, lo que 
hace \emph{WebPack} es buscar esa instrucción y juntar en un mismo fichero todas las dependencias (import) que
se utilicen a partir de los ficheros iniciales.\\\\

\begin{figure}[h]
	\centering
	\includegraphics[width=8cm, height=3cm]{img/output-webpack.PNG}
	\caption{Nombre y directorio de los ficheros de salida empaquetados por \emph{WebPack}.}
	\label{fig:output}
\end{figure}

La figura ~\ref{fig:output} muestra dónde y qué nombre tendrán los ficheros empaquetados por la herramienta
\emph{WebPack}. Cabe explicar la propiedad '[name]', que declara que el nombre del fichero de salida ha de ser
el nombre de la etiqueta usado en \emph{entry} que se muestra en la figura ~\ref{fig:entry}. En este caso 
tenemos \emph{websim.bundle.js}.

\begin{figure}[h]
	\centering
	\includegraphics[width=5cm, height=2cm]{img/resolve-webpack.PNG}
	\caption{Declaración de las extensiones que ha de buscar \emph{WebPack}.}
	\label{fig:resolve}
\end{figure}

La Figura ~\ref{fig:resolve} muestra la configuración en \emph{WebPack} que declara cómo han de ser resueltos los
módulos. Por ejemplo si hacemos uso de la instrucción \emph{import 'A-FRAME'} lo que hace es buscar la
dependencia con el nombre 'A-FRAME' y extensión declarada en la configuración.

\begin{figure}[h]
	\centering
	\includegraphics[width=5cm, height=3cm]{img/devserver-webpack.PNG}
	\caption{Configuración del servidor estático para desarrollo que \emph{WebPack}
	permite usar.}
	\label{fig:devserver}
\end{figure}

La Figura ~\ref{fig:devserver} muestra la configuración de puertos y la dirección IP de la máquina donde 
se va a levantar un servidor estático de desarrollo por la herramienta \emph{WebPack}. Esto permite hacer
cambios en el 'core' de la aplicación y realizar el empaquetado y servicio de la aplicación de manera 
automática. Permite acelerar el proceso de desarrollo de cualquier aplicación. Como se ha mencionado,
es un servidor estático por lo tanto tenemos que tener una página \emph{index.html} que será la que se
servirá en el navegador por defecto y ésta ha de hacer uso de los ficheros empaquetados por \emph{WebPack}.

\begin{figure}[h]
	\centering
	\includegraphics[width=5cm, height=5cm]{img/module-webpack.PNG}
	\caption{Configuración de los módulos de preprocesado a usar por \emph{WebPack}.}
	\label{fig:module}
\end{figure}

La Figura ~\ref{fig:module} muestra la configuración para los módulos de preprocesado de los ficheros
a empaquetar, permite declarar cuál va a ser el paquete utilizado (que en este caso será \emph{babel-loader}) y
las extensiones de los ficheros que se van a usar, en este caso \emph{.js y .jsx (React)}. La extensión '.jsx'
no va a ser utilizada. Esto nos permite compilar y transformar archivos en \emph{TypeScript} por ejemplo a JavaScript para ser 
empaquetado, hay más tipos de \emph{loaders} para hacer empaquetado incluso de archivos CSS.\\

\subsection{Instalación}

Para la instalación en local es necesario tener instalados NodeJS y NPM, para ello hay que abrir
una consola (Ubuntu/Linux) o un CMD (Windows)

\begin{itemize}
	\item Ubuntu/Linux:
	\begin{lstlisting}[language=bash]
  		$ sudo apt update
  		$ sudo apt get install nodejs
  		$ sudo apt get install npm
  		
  		-- Para comprobar que Node está instalado:
  		
  		$ nodejs --version
	\end{lstlisting}
	
	\item Windows: Descarga el instalador de NodeJS de su página oficial.
\end{itemize}

Una vez instalado se copia el repositorio que se quiera utilizar de los dos posibles \footnote{\url{https://github.com/RoboticsURJC-students/2018-tfg-alvaro_paniagua}} 
\footnote{\url{https://github.com/JdeRobot/WebSim}}.


Moverse a la carpeta del repositorio que se acaba de clonar y poner lo siguiente:

\begin{lstlisting}[language=bash]
$ npm install
\end{lstlisting}

Esto instalará todas las dependencias en la carpeta 'node-modules', una vez finalizado esto se podrá
arrancar el servidor, para ello hay dos opciones.

\begin{itemize}
	\item Python: Necesitamos tener instalado python en nuestro equipo, si lo tenemos instalado
	ejecutamos la siguiente instrucción en la línea de comandos:
	\begin{lstlisting}[language=bash]
	-- Python v.3
	$ python -m http.server [port]
	-- Python v.2
	$ python -m SimpleHTTPServer [port]
	\end{lstlisting}

	\item NodeJS: En ambos repositorios anteriormente mencionados se ofrece bajo el nombre \emph{server.js} un
	servidor que sirve recursos estáticos, basta con ejecutar lo siguiente tanto en Windows como Ubuntu.
	\begin{lstlisting}[language=bash]
	$ node server.js
	\end{lstlisting}
\end{itemize}

Una vez ejecutadas cualquiera de las dos opciones abrimos el navegador, se recomienda Firefox debido a que A-FRAME tiene compatibilidad total con éste e ingresamos la siguiente URL \url{localhost:8000/}.


En general, cualquier servidor de ficheros estático valdría para servir las aplicaciones ya que se han creado
de tal manera que el servidor usado no tenga ninguna funcionalidad.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% ROBÓTICA EDUCATIVA %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\cleardoublepage
\chapter{Robótica educativa con \emph{WebSim}}
\label{chap:robotica-educativa}

Como se ha comentado en el capítulo ~\ref{chap:intro}, los usos del simulador propuesto
van orientados a la robótica educativa. Se han creado dos aplicaciones web docentes de manera que los usuarios aprenderán a programar la lógica del robot usando el acceso simple a los sensores y actuadores del robot simulado que proporciona \emph{WebSim}. Para el uso de ambas aplicaciones web podemos instalarlas en local o bien se 
puede acceder a la siguiente URL \footnote{\url{https://roboticsurjc-students.github.io/2018-tfg-alvaro_paniagua/}} \footnote{\url{https://jderobot.github.io/WebSim/}}. En ellas se han realizado una serie de ejercicios típicos de enseñanza de
robótica ya establecidos previamente al
proyecto. 

Es necesario remarcar que \emph{A-Frame} está soportado en los siguientes
navegadores y en ellos funcionan correctamente las dos aplicaciones web que aquí se describen. Los navegadores soportados son: Firefox 55+ para Windows, Supermedium, Chromium experimental, Navegador Oculus (GearVR), 
Samsung Internet (GearVR), Microsoft Edge, (Móvil) Safari para iOS, (Móvil) Chrome para Android, (Móvil) Firefox para iOS,
(Móvil) Samsung Internet, (Móvil) Navegador UC.8 Los navegadores móviles no tienen soporte VR oficial por lo tanto el rendimiento será notablemente peor que en el resto. Pese a estar soportado en varios navegadores se recomienda su uso en Firefox 
siempre que así sea posible.


\section{Programando aplicaciones robóticas en \emph{JavaScript} con \emph{WebSim}}
\label{sec:app-js}

La primera de las dos aplicaciones web mencionadas hace uso del simulador
\emph{WebSim} y el editor ACE Editor. En ese editor el alumno puede utilizar el HAL API del robot simulado en
lenguaje JavaScript para programar la lógica del robot y resolver un ejercicio propuesto.\\


En la figura ~\ref{fig:websim-js} se muestra la arquitectura de la aplicación web y cómo el editor de código
se comunica con WebSim que se encarga de interpretar el código del programador.

\begin{figure}[H]
	\centering
	\includegraphics[width=13cm, height=7cm]{img/interfaz-js.PNG}
	\caption{Interfaz de usuario de WebSim + editor JavaScript (Ace Editor).}
	\label{fig:js-ui}
\end{figure}

\begin{figure}[h]
	\centering
	\includegraphics[width=14cm, height=9cm]{img/Websim-JS.PNG}
	\caption{Arquitectura de la aplicación que hace uso del editor ACE y WebSim.}
	\label{fig:websim-js}
\end{figure}


Como se ve en la figura ~\ref{fig:websim-js} la arquitectura tiene dos módulos, uno para el editor y el módulo de WebSim lo que 
dota de flexibilidad a la hora del desarrollo permitiendo tener distintos editores distintos. El módulo del editor se encarga principalmente de crear los manejadores de eventos para los eventos de los
botones empotrados en la página web, configurar el renderizado del editor y la obtención de código escrito por 
el usuario.

La figura ~\ref{fig:js-ui} muestra la interfaz de usuario de la aplicación web que utiliza de \emph{WebSim}
y el editor de código \emph{ACE Editor}. El usuario hará uso del HAL API del robot simulado y escribirá la lógica en la parte derecha de la imagen (editor). Una vez programada la lógica lo único necesario para 
ejecutar es darle al botón \emph{Play} situado en la parte superior izquierda del editor.
El flujo de funcionamiento de la aplicación sigue los pasos marcados en la figura 
~\ref{fig:websim-js}.

\begin{itemize}
	\item (1): El usuario crea un código haciendo uso directo de la variable \emph{myRobot} y pulsa el botón
	de ejecutar en la aplicación. 
	
	\item (2): El evento 'click' es recogido por el módulo del editor que llama a la funcionalidad necesaria 
	para obtener el código que el usuario ha escrito en el editor empotrado y lo almacena en una variable.
	
	\item (3): Una vez se ha recogido el código en el paso (2) el módulo editor envía un evento que el 
	módulo WebSim se encuentra escuchando. El código llega al módulo WebSim y este se encarga de ejecutarlo
	o no dependiendo de si ya existe un código corriendo en el robot simulado.
\end{itemize}



\section{Ejercicios en \emph{JavaScript}}
\label{sec:ejercicios-js}

Se han programado tres ejercicios típicos que se utilizan en la 
plataforma \emph{Kibotics} para enseñar a los alumnos programación de robots y visión artificial.
Como tenemos una aplicación web con un editor de código \emph{JavaScript} se pueden incluso crear
otros ejercicios ampliando el espectro de aprendizaje para los alumnos. Por ejemplo conseguir
mover el robot mediante teclado, lo que les lleva a conocer la orientación a eventos de \emph{JavaScript}
y el manejo de estos, en concreto los eventos del teclado.\\

Se ha desarrollado una solución de referencia para tres ejercicios clásicos en robótica educativa que se explican en las siguientes subsecciones. Los ejercicios únicamente se diferencian en los elementos externos de
la escena (cajas, esferas, texturas del suelo, etc.) y en los sensores y actuadores que se usan para su
resolución.


\subsection{Ejercicio \emph{Sigue Líneas}}

Trata de crear la lógica interna del robot con el objetivo que siga la línea que 
describe un circuito haciendo uso de los sensores infrarrojos en el robot simulado. Para ello se emplean el
método del HAL API \emph{readIR} y los métodos de los motores \emph{setV y setW}.

\begin{figure}[h]
	\centering
	\includegraphics[width=11cm, height=8cm]{img/followline-codeJS.PNG}
	\caption{Lógica programada en la aplicación web para resolver el ejercicio \emph{Sigue Lineas}.}
	\label{fig:fl-code-js}
\end{figure}

Como se aprecia en la Figura ~\ref{fig:fl-code-js} la lógica es simple. En el código se pueden hacer distintas mejoras como condiciones de retorno si el robot sale de la línea. A continuación, en la Figura 
~\ref{fig:fl-execution} se muestran varios fotogramas con la ejecución del código de la Figura 
~\ref{fig:fl-code-js}.

\begin{figure}[H]
	\centering
	\includegraphics[width=15cm, height=4cm]{img/followline-frame-1.PNG}
	\includegraphics[width=15cm, height=4cm]{img/followline-frame-2.PNG}
	\includegraphics[width=15cm, height=4cm]{img/followline-frame-3.PNG}
	\caption{Ejecución del algoritmo del \emph{Sigue Lineas}.}
	\label{fig:fl-execution}
\end{figure}

\subsection{Ejercicio \emph{Evitar Obstáculos}}

Este ejercicio consiste en programar la lógica del robot utilizando los sensores de obstáculos y los motores para que el robot sea capaz de moverse de manera autónoma en el entorno evitando chocar con los
distintos objetos de la escena. Una aplicación práctica similar de esta lógica es el conocido aspirador autónomo \emph{Roomba}.

\begin{figure}[h]
	\centering
	\includegraphics[width=10cm, height=7cm]{img/hitturn-code-js.PNG}
	\caption{Lógica programada para resolver el ejercicio \emph{Evitar Obstáculos}.}
	\label{fig:hitturn-code-js}
\end{figure}

La lógica del comportamiento es básica, el robot avanza hasta que uno de los sensores de obstáculos detecta 
un objeto cuya distancia respecto al robot se guarda en la variable \emph{distancias}. Se comprueba si las 
distancias guardadas en esa variable son menores a un umbral que establece el usuario, en este caso
es 1 metro. Si la condición se cumple el robot deja de avanzar para girar hasta que se haya evitado el obstáculo.

\begin{figure}[H]
	\centering
	\includegraphics[width=15cm, height=4cm]{img/hitturn-frame-1.PNG}
	\includegraphics[width=15cm, height=4cm]{img/hitturn-frame-2.PNG}
	\includegraphics[width=15cm, height=4cm]{img/hitturn-frame-3.PNG}
	\caption{Ejecución del algoritmo del \emph{Evita Obstáculos}.}
	\label{fig:hitturn-execution}
\end{figure}


\subsection{Ejercicio \emph{Sigue Objeto}}

El ejercicio \emph{Sigue Objeto} es una mejora del ejercicio \emph{Sigue Linea} en la cual se hace uso del 
sensor más potente que posee el robot simulado, la cámara. Es un ejercicio que permite poner en práctica 
conocimientos sobre visión artificial y tratamiento de imagen de manera muy sencilla. Para la resolución del ejercicio se hace 
uso del método del HAL API del robot simulado \emph{getObjectColor} o de \emph{getObjectColorRGB} que ofrecen 
las coordenadas en la imagen del centro del objeto detectado con el color que el usuario especificado como parámetro
a la función. Haciendo uso de éstas coordenadas el alumno ha de ser capaz de programar la lógica necesaria 
para no perder de vista el objeto. Además, al usar el área que devuelve la función podemos saber si el objeto
y el robot se acercan o se alejan (si el área es mayor que en la iteración anterior se están acercando y si es menor se están alejando).
Como en los ejercicios anteriores se hace uso de los motores.

\begin{figure}[h]
	\centering
	\includegraphics[width=10cm, height=7cm]{img/followball-code-js.PNG}
	\caption{Lógica programada en la aplicación web para resolver el ejercicio \emph{Sigue Objeto}.}
	\label{fig:fball-code-js}
\end{figure}

Como se ve en la Figura ~\ref{fig:fball-code-js} se configura una velocidad de giro en función de dónde
se encuentra el centro de la esfera en la cámara del robot. El algoritmo mostrado es un algoritmo 
básico, este ejercicio permite además algoritmos más sofisticados en los cuales la velocidad de giro
variará con la posición del centro de la esfera en la de modo que la velocidad de giro será mayor en función de si la esfera se encuentra en los extremos de la
imagen. Este algoritmo proporcional se adapta mejor al caso en el cuál la esfera en la escena se encuentre en movimiento.



\begin{figure}[H]
	\centering
	\includegraphics[width=15cm, height=4cm]{img/followball-frame-1.PNG}
	\includegraphics[width=15cm, height=4cm]{img/followball-frame-2.PNG}
	\includegraphics[width=15cm, height=4cm]{img/followball-frame-3.PNG}
	\caption{Ejecución del algoritmo del \emph{Sigue Objeto}.}
	\label{fig:fball-execution}
\end{figure}

Como se aprecia en la figura ~\ref{fig:fball-execution}, este ejercicio tiene un gran peso ya que se puede incrementar la dificultad añadiendo 
movimiento a la esfera de manera constante o variable.

\section{Programando aplicaciones robóticas en \emph{Scratch} con \emph{WebSim}}
\label{sec:app-blockly}

Haciendo uso del mismo núcleo de \emph{WebSim} se ha creado una aplicación web para la resolución de
los ejercicios planteados en \emph{Kibotics} mediante el uso de bloques visuales con 
\emph{Blockly}.\\

Esta aplicación web se ha creado para una toma de contacto con la programación de robots para 
usuarios no familiarizados en el lenguaje \emph{JavaScript}. En la figura ~\ref{fig:interfaz-blockly}
se muestra la interfaz de bloques visuales en la que se ve que no es necesario programar con un lenguaje de texto, es una 
interfaz de tipo \emph{Plug and Play} en la que se conectan los bloques gráficamente unos con otros para
generar código.

\begin{figure}[h]
	\centering
	\includegraphics[width=16cm, height=9cm]{img/interfaz-blockly.PNG}
	\caption{Interfaz de la aplicación WebSim + Blockly}
	\label{fig:interfaz-blockly}
\end{figure}

\begin{figure}[h]

	\centering
	\includegraphics[width=14cm, height=10cm]{img/Websim-Blockly.PNG}
	\caption{Arquitectura de la aplicación web que hace uso de Websim + Blockly.}
	\label{fig:websim-blockly}
\end{figure}

En la figura ~\ref{fig:websim-blockly} se muestra la arquitectura usada para la construcción de esta aplicación
web. Como se puede apreciar, tanto esta como la arquitectura en la figura ~\ref{fig:websim-js} son similares. 
Los únicos módulos que cambian son los referentes al editor, cada editor necesitará una configuración 
diferente en función de sus características.

El flujo de ejecución de código es el mismo que el mencionado en la sección ~\ref{sec:ejercicios-js} pero 
modificando el módulo del editor de manera que incluya los pasos necesarios para configurar Blockly y traducir los bloques al 
lenguaje JavaScript.

Al estar usando Blockly necesitamos crear algunos bloques personalizados que, al ser empleados por el usuario, se 
traducirán a las instrucciones JavaScript equivalentes que hacen uso del HAL API que ofrece WebSim. Estos bloques específicos necesitan
tres 'piezas' para ser utilizados correctamente:

\begin{itemize}
	\item JSON de configuración, en el que se configuran los parámetros de entrada, el texto en el bloque, el 
	color del bloque y todos los atributos necesarios. Por simplicidad se han usado las herramientas de 
	desarrollo que ofrece Google en las cuales se permite generar un bloque personalizado a partir de otros 
	bloques. 

		\begin{figure}[H]
			\centering
			\includegraphics[width=9cm, height=8cm]{img/custom-block-json.PNG}
			\caption{JSON de configuración de un bloque personalizado de \emph{Blockly}.}
			\label{fig:custom-block-json}
		\end{figure}

	\item Función para iniciación del bloque, es necesaria una función que muestre el bloque en el menú
	del editor de código embebido en la página web. La Figura ~\ref{fig:custom-block-init} muestra un ejemplo
	de dicha función.
		\begin{figure}[H]
			\centering
			\includegraphics[width=9cm, height=3cm]{img/custom-block-init.PNG}
			\caption{Función para que el bloque personalizado de \emph{Blockly} se muestre en el menú del editor
			embebido.}
			\label{fig:custom-block-init}
		\end{figure}
	
	\item Traductor del bloque a JavaScript (puede haber otros traductores a otros lenguajes como Python, PHP).
	Cuando generamos el código a partir de la instrucción \emph{workspaceToCode} de \emph{Blockly}, 
	internamente lo que hace \emph{Blockly} es llamar a la función que traduce el código al lenguaje necesario
	para cada uno de los bloques usados en editor embebido. La Figura ~\ref{fig:custom-block-generator} 
	muestra un ejemplo de dicha función.
	\begin{figure}[H]
			\centering
			\includegraphics[width=16cm, height=3.5cm]{img/custom-block-generator.PNG}
			\caption{Función que genera el código JavaScript de un bloque personalizado de \emph{Blockly}.}
			\label{fig:custom-block-generator}
		\end{figure}
\end{itemize}


\section{Ejercicios en \emph{Scratch}}
\label{sec:ejercicios-scratch}

Los ejercicios propuestos en esta aplicación web son los mismos a los propuestos en la sección ~\ref{sec:ejercicios-js} pero en este
caso se programa a través de bloques visuales. La diferencia entre los tres ejercicios reside principalmente en la escena que se va a usar, los métodos necesarios
para su resolución y la plantilla de bloques visuales que se proporciona al usuario.\\

En la figura ~\ref{fig:interfaz-blockly} se muestra la escena necesaria para el ejercicio de perseguir la esfera de color verde. Para que la esfera se mueva necesita importar un \emph{script} en el navegador en el cual 
dote de movimiento aleatorio a la esfera.\\

Estos tres ejercicios permiten validar el desarrollo del proyecto ya que permiten comprobar la estabilidad
y flexibilidad del simulador y más concretamente del HAL API desarrollado para el robot.

\subsection{Ejercicio \emph{Sigue Líneas}}

Como se ha explicado en la sección ~\ref{sec:ejercicios-js}, este ejercicio hace uso del método
\emph{readIR} del \emph{HAL API} del robot simulado. Se usan además bloques de \emph{Blockly} personalizados
para hacer uso de el método del \emph{HAL API}. El código resultante es el de la Figura 
~\ref{fig:fl-scratch-code}:

\begin{figure}[h]
	\centering
	\includegraphics[width=15cm, height=5.5cm]{img/followline-scratch-code.PNG}
	\caption{Bloques de \emph{Scratch} que implementan la lógica del algoritmo \emph{Sigue Líneas}.}
	\label{fig:fl-scratch-code}
\end{figure}

Como se aprecia los bloques tienen una estructura similar al código
generado en JavaScript en el apartado análogo de la sección ~\ref{sec:ejercicios-js} pero al estar usando
bloques visuales el usuario no se tiene que preocupar de la sintaxis, únicamente de la estructura.

\begin{figure}[H]
	\centering
	\includegraphics[width=16cm, height=3cm]{img/followline-scratch-frame1.PNG}
	\includegraphics[width=16cm, height=3cm]{img/followline-scratch-frame2.PNG}
	\includegraphics[width=16cm, height=3cm]{img/followline-scratch-frame3.PNG}
	\caption{Ejecución del algoritmo del \emph{Sigue Objeto}.}
	\label{fig:fl-scratch-execution}
\end{figure}

\subsection{Ejercicio \emph{Evitar Obstáculos}}

Para la resolución de este ejercicio ha sido necesario crear bloques personalizados como los siguientes:

\begin{figure}[h]
	\centering
	\includegraphics[width=6cm, height=0.8cm]{img/getdistance-block.PNG}
	\includegraphics[width=6cm, height=0.8cm]{img/getdistances-block.PNG}
	\caption{Bloques personalizados de \emph{Blockly} que hacen uso de los métodos \emph{getDistance} y
	\emph{getDistances} del \emph{HAL API} del robot simulado.}
	\label{fig:customblocks}
\end{figure}

El bloque a la izquierda devuelve un valor numérico en metros con la distancia al obstáculo detectado por
el sensor de obstáculos justo delante del robot simulado. El bloque de la izquierda es similar al anterior
pero devuelve una lista con valores numéricos debido a que el campo visual del sensor es de 180º.

\begin{figure}[H]
	\centering
	\includegraphics[width=15.5cm, height=3.7cm]{img/hitturn-code-scratch.PNG}
	\caption{Estructura de bloques de \emph{Scratch} que resuelve el ejercicio \emph{Evitar Obstáculos}.}
	\label{fig:ht-scratch-code}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=15.5cm, height=4cm]{img/hitturn-scratch-frame-1.PNG}
	\includegraphics[width=15.5cm, height=4cm]{img/hitturn-scratch-frame-2.PNG}
	\includegraphics[width=15.5cm, height=4cm]{img/hitturn-scratch-frame-3.PNG}
	\caption{Ejecución del algoritmo para el ejercicio \emph{Evitar Obstáculos}.}
	\label{fig:ht-scratch-execution}
\end{figure}

\subsection{Ejercicio \emph{Sigue Objeto}}

Como para los ejercicios anteriores, ha sido necesario generar algunos bloques personalizados de \emph{Blockly} adicionales. En este caso se han
generado bloques que permiten hacer uso del \emph{HAL API} de la cámara del robot simulado. El bloque generado se muestra en la
Figura ~\ref{fig:custom-block}.

\begin{figure}
	\centering
	\includegraphics[width=12cm, height=1cm]{img/getObjColor-block.PNG}
	\caption{Bloque que hace uso del método \emph{getObjectColor} del \emph{HAL API} del robot simulado.}
	\label{fig:custom-block}
\end{figure}


\begin{figure}[H]
	\centering
	\includegraphics[width=16cm, height=4.5cm]{img/followball-code-scratch.PNG}
	\caption{Estructura de bloques de \emph{Scratch} que resuelve el ejercicio \emph{Sigue Objeto}.}
	\label{fig:followball-scratch-code}
\end{figure}


\begin{figure}[H]
	\centering
	\includegraphics[width=15.5cm, height=4cm]{img/followball-scratch-frame-1.PNG}
	\includegraphics[width=15.5cm, height=4cm]{img/followball-scratch-frame-2.PNG}
	\includegraphics[width=15.5cm, height=4cm]{img/followball-scratch-frame-3.PNG}
	\caption{Ejecución del algoritmo para el ejercicio \emph{Sigue Objeto}.}
	\label{fig:followball-scratch-execution}
\end{figure}


Como conclusión se puede apreciar que los algoritmos en \emph{JavaScript} y en \emph{Scratch} son los mismos o muy parecidos,
la gran diferencia reside en que el usuario de Scratch no necesita conocer el lenguaje ya que los bloques son autodescriptivos
y muy intuitivos.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% CONCLUSIONES %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\cleardoublepage
\chapter{Conclusiones}
\label{chap:conclusiones}

Finalmente, una vez descrito todo el software desarrollado a lo largo de este Trabajo Fin de Grado, en este capítulo se recopilan las conclusiones alcanzadas, se valoran las contribuciones realizadas y se plantean posibles líneas de mejora y extensión.

\section{Valoración final}
\label{sec:valoracion_objetivo_final}

Al repasar los objetivos del capítulo ~\ref{chap:cap-objetivos} concluimos que se ha conseguido llevar a 
cabo todos los puntos establecidos. El principal subobjetivo era crear un simulador en lado cliente y se ha diseñado, desarrollado y 
validado experimentalmente el simulador \emph{WebSim} a través de la resolución de los ejercicios planteados en el capítulo
~\ref{chap:robotica-educativa} en las aplicaciones web mencionadas en el mismo. Como
se ha podido intuir a lo largo del documento no es necesario un servidor con una gran funcionalidad y coste computacional, al 
contrario, el servidor utilizado es un servidor estático simple alojado en la plataforma de \emph{Github}.\\

El segundo y tercer subobjetivos también se han satisfecho exitosamente con la creación de sendas aplicaciones web que utilizan WebSim y ofrecen al usuario la posibilidad de programar al robot simulado en JavaScript o en el lenguaje visual de bloques Scratch.


\section{Aplicación de lo aprendido en el grado}
\label{sec:aplicacion_aprendido}

Para llevar acabo este proyecto me han sido imprescindibles los conocimientos adquiridos en las 
asignaturas relacionadas con la programación y tratamiento de imagen, a continuación se enumeran
las asignaturas relacionadas con estos campos cursadas en el Grado.

\begin{itemize}
	\item Informática I con el lenguaje 'Picky' en el cual tuve una primera toma de contacto con los
	fundamentos de programación.
	
	\item Informática II, continuación de Informática I en la cual se llevaron a cabo proyectos algo más
	elaborados y se profundizó en el aprendizaje de 'punteros'.
	
	\item Protocolos de Transmisión de Audio y Vídeo en Internet, esta asignatura fue la primera 
	aproximación con un lenguaje de programación orientado a objetos como \emph{Python}.
	
	\item Graficos y visualización 3D, esta asignatura ha sido una de las claves de aprendizaje para el
	desarrollo del proyecto ya que fue mi primera toma de contacto con el lenguaje \emph{JavaScript} y 
	el \emph{canvas}.
	
	\item Laboratorios en Tecnologías y Aplicaciones Web, el segundo punto clave de aprendizaje ya que
	estableció la base de conocimiento sobre tecnologías web como NodeJS y Django.
	
	\item Tratamiento digital de la imagen, esta asignatura ha sido importante debido a que ha 
	establecido la base de conocimiento sobre filtrado de imagen necesario para su uso con el robot.
\end{itemize}


Durante el proyecto he aprendido muchísimo sobre tecnologías web como el uso de herramientas de 
empaquetado como Webpack, he mejorado mucho en el uso de control de versiones en la plataforma Github.
Además he aprendido cómo estructurar una aplicación completa mediante el uso de módulos de código 
separados. Por último he aprendido cómo usar los paquetes \emph{NPM} como dependencias de código
lo que simplifica mucho la utilización de la aplicación e instalación de dependencias.



\section{Mejoras futuras}
\label{sec:mejoras_futuras}

Como futuras mejoras hay muchas por abarcar, a continuación se enumeran algunas de ellas:

\begin{itemize}
	\item Permitir que \emph{WebSim} utilice un fichero de configuración para crear el robot y el mundo simulado. Esto abre
	paso a manejar en el simulador varios tipos de robots y escenarios. Se buscará la compatibilidad de los ficheros empleados
	con otros simuladores como Gazebo.
	\item Añadir funcionalidad de guardado de código en el servidor para los alumnos que permitirá extender la plataforma a
	un gran conjunto de usuarios. Además se integrará en Kibotics.
	\item Extender el soporte para otros robots con distinta funcionalidad, por ejemplo drones.
	\item Mejora general de la escena de A-Frame.
	\item Integrar las aplicaciones web con el robot PiBot real, de modo que el código del editor (ACE o Blockly) pueda 
	descargarse y ejecutarse en el robot físico.
	\item Explorar mejoras de rendimiento.
	\item Explorar portabilidad a otros navegadores como Chrome, Safari, etc. Aunque algunos de ellos
	no soportan el entorno A-Frame todavía.
\end{itemize}

Además, hay muchas características 
interesantes que se podrán ir implementando a posteriori como la integración de WebSim con Electron para crear una aplicación de 
escritorio o hacer uso de otros lenguajes de programación como Python en las aplicaciones de los usuarios conectándolas igualmente a WebSim.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% BIBLIOGRAFIA %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\cleardoublepage

% Las siguientes dos instrucciones es todo lo que necesitas
% para incluir las citas en la memoria
% memoria.bib es el nombre del fichero que contiene
% las referencias bibliográficas. Abre ese fichero y mira el formato que tiene,
% que se conoce como BibTeX. Hay muchos sitios que exportan referencias en
% formato BibTeX. Prueba a buscar en http://scholar.google.com por referencias
% y verás que lo puedes hacer de manera sencilla.
% Más información:
% http://texblog.org/2014/04/22/using-google-scholar-to-download-bibtex-citations/
\begin{thebibliography}{7}


\bibitem{Blockly}
\textit{Herramientas de desarrollo de Google Blockly:}
\url{https://blockly-demo.appspot.com/static/demos/blockfactory/index.html}


\bibitem{JavaScript}
\textit{Historia JavaScript:}
\url{https://medium.com/@benastontweet/lesson-1a-the-history-of-javascript-8c1ce3bffb17}


\bibitem{WebPack}
\textit{Documentación Webpack:}
Documentación oficial:
\url{https://webpack.js.org/}

Vídeo introductorio de configuración:
\url{https://www.youtube.com/watch?v=PakrjWSD6Mo}

Explicación configuración WebPack: 
\url{https://github.com/jgbarah/aframe-playground/blob/master/figures-03/README.md}


\bibitem{HTML}
\textit{Documentación HTML:}
\url{https://www.computerhope.com/jargon/h/html.htm}


\bibitem{NPM}
\textit{Introducción a NPM:}
\url{https://www.impressivewebs.com/npm-for-beginners-a-guide-for-front-end-developers/}

\bibitem{A-Frame}
\textit{Documentación completa A-Frame:}
\url{https://aframe.io/}

\bibitem{jQuery}
\textit{Documentación jQuery:}
\url{https://jquery.com/}


\end{thebibliography}
\end{document}
